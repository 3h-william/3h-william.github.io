<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>3h_William</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description">
<meta property="og:type" content="website">
<meta property="og:title" content="3h_William">
<meta property="og:url" content="http://yoursite.com/index.html">
<meta property="og:site_name" content="3h_William">
<meta property="og:description">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="3h_William">
<meta name="twitter:description">
  
    <link rel="alternative" href="/atom.xml" title="3h_William" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  <link rel="stylesheet" href="/css/style.css" type="text/css">
  

</head>
<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">3h_William</a>
      </h1>
      
        <h2 id="subtitle-wrap">
          <a href="/" id="subtitle">Hello Sunshine</a>
        </h2>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" results="0" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://yoursite.com"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main">
  
    <article id="post-zookeeper_stuck_dns_network" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2015/11/10/zookeeper_stuck_dns_network/" class="article-date">
  <time datetime="2015-11-10T01:18:11.000Z" itemprop="datePublished">2015-11-10</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2015/11/10/zookeeper_stuck_dns_network/">使用Zookeeper过程中遇到客户端stuck的问题和解决建议</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="背景">背景</h1><p>最近在查询一个和zookeeper相关的case。追查结果是一个称之为”Reverse DNS lookup”的问题。<br>可以称之为:反向DNS查找。 <a href="https://en.wikipedia.org/wiki/Reverse_DNS_lookup" target="_blank" rel="external">https://en.wikipedia.org/wiki/Reverse_DNS_lookup</a></p>
<h1 id="隐患">隐患</h1><p>zookeeper 3.5以下的版本，会在客户端并发链接的时候，有一定几率发生stuck，随着并发连接的增多，发生概率越大。尤其是第一次创建连接的时候。<br>(因为zookeeper的client版本和server的版本兼容性，可以先升级客户端，客户端版本&gt;服务端版本，是支持的)</p>
<p>原因:<br>-&gt; 1)   归根结底，是因为DNS解析的问题。 可参考:<br><a href="https://issues.apache.org/jira/browse/ZOOKEEPER-1666" target="_blank" rel="external">issue 1891</a>   </p>
<p>-&gt; 2)   1366 issue在3.4.6已经fix，但是和它相关的另一个issue，只有在3.5才fix<br><a href="https://issues.apache.org/jira/browse/ZOOKEEPER-1891" target="_blank" rel="external">issue 1891</a>  </p>
<h1 id="案例与解释">案例与解释</h1><p>简单的来说，问题出在 “通过IP找HostName”，上， 例如: 通过”10.x.x.x”找到”hadoopXXX” 这个方法上。<br>在计算机网络中，我们可以用ip也可以用HostName来表明一台唯一的机器，我们也知道有一个叫做DNS的服务来帮助我们将IP转化为hostName. <strong>但是，这一个转换是有开销的</strong>。<br>是需要通过上层的DNS服务器或者IP机器本身的网卡来达到映射。<br>至于细节，可以参考wiki  </p>
<p>接下来，我们可以本地来模拟这个过程:<br>我们用Java可以启动以下代码，其中，注释的两行是两种构建方式。  </p>
<figure class="highlight lisp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">        for<span class="list">(<span class="keyword">int</span> i=0<span class="comment">;i&lt;100;i++)&#123;</span></span><br><span class="line">            new Thread<span class="list">(<span class="keyword">new</span> Runnable<span class="list">()</span> &#123;</span><br><span class="line">                @Override</span><br><span class="line">                public void run<span class="list">()</span> &#123;</span><br><span class="line">//   <span class="number">1</span>                 InetSocketAddress ia = new InetSocketAddress<span class="list">( <span class="string">"10.x.x.x"</span>,<span class="number">2181</span>)</span><span class="comment">;;</span></span><br><span class="line">//   <span class="number">2</span>                 InetSocketAddress ia = new InetSocketAddress<span class="list">( <span class="string">"hadoopXXX/10.x.x.x"</span>,<span class="number">2181</span>)</span><span class="comment">;</span></span><br><span class="line">                    System.out.println<span class="list">(<span class="keyword">ia</span>.getHostName<span class="list">()</span>)</span><span class="comment">;</span></span><br><span class="line">                &#125;</span><br><span class="line">            &#125;)</span>.start<span class="list">()</span><span class="comment">;</span></span><br><span class="line">        &#125;</span></span><br></pre></td></tr></table></figure>
<ul>
<li>当使用注释第一行的时候，线程是有很大的几率发生stuck.  </li>
<li>当使用注释第二行的时候，是不会有这个问题。<br>而不凑巧对的是，zookeeper3.5以下的版本中，是存在第一行注释的代码的。因此，问题就产生了。  </li>
</ul>
<hr>
<p>作者：3h-william<br>联系: <a href="https://github.com/3h-william" target="_blank" rel="external">https://github.com/3h-william</a><br>出处：<a href="http://3h-william.github.io" target="_blank" rel="external">http://3h-william.github.io</a>  </p>
<hr>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2015/11/10/zookeeper_stuck_dns_network/" data-id="cigspks7g0003uogt9rvgah5c" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/zookeeper/">zookeeper</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-sqoop_orc" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2015/09/11/sqoop_orc/" class="article-date">
  <time datetime="2015-09-11T07:00:11.000Z" itemprop="datePublished">2015-09-11</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2015/09/11/sqoop_orc/">使用sqoop生成ORC格式的文件，并给Hive加载</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="背景">背景</h1><p>在当前版本: sqoop-1.4.5(CDH540)中的sqoop并不支持导出格式为orcfile的格式。因此，如果我们想将数据从DB=&gt;HDFS，并生成ORC格式，只有两个办法。</p>
<p>-&gt; a) 使用HCatalog服务，目前没有测试过，看文档是可以生成ORC的格式。<br>-&gt; b) 原始办法，通过Hive的insert命令将一张原本不是ORC格式的数据，导入到另一个ORC格式中，这样就有了ORC格式的文件。 但该方法会造成时间、资源消耗翻倍。</p>
<h1 id="DIY">DIY</h1><p>既然如此，自己动手，丰衣足食。介于近年来对于sqoop的使用经验，很方便的定位到了几个地方，并加入了一些类和方法，最终达到了目的。当然，因为我们的需求目前仅仅是从DB = &gt; HDFS，即只是增加了import方法中的参数，并没有在export =&gt; DB中增加。</p>
<p>具体代码可以参考我已经提供的版本 <a href="https://github.com/3h-william/sqoop-orc-import" target="_blank" rel="external">link</a><br>使用ant编译后，替换原来的sqoop jar即可。</p>
<h1 id="关键点">关键点</h1><p>-&gt; 1) 使用hive的提供的TypeInfoUtils 构建ORC file的Schema.并传递到每一个Map中<br>-&gt; 2) 使用 org.apache.hadoop.hive.ql.io.orc.OrcNewOutputFormat 作为输出格式</p>
<hr>
<p>作者：3h-william<br>联系: <a href="https://github.com/3h-william" target="_blank" rel="external">https://github.com/3h-william</a><br>出处：<a href="http://3h-william.github.io" target="_blank" rel="external">http://3h-william.github.io</a>  </p>
<hr>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2015/09/11/sqoop_orc/" data-id="cigspks7j0006uogt2dlerto9" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/hive/">hive</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/sqoop/">sqoop</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-hbase_balance_problem" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2015/09/08/hbase_balance_problem/" class="article-date">
  <time datetime="2015-09-08T06:08:11.000Z" itemprop="datePublished">2015-09-08</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2015/09/08/hbase_balance_problem/">HBase Coprocessor问题引起异常Balance</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="问题背景">问题背景</h1><p>最近发现HBase集群的文件compaction次数明显增加。经常到时M/R job扫描snapshot文件的时候发生文件找不到，为此trace了一下日志，大致发现了问题。<br><img src="/img/hbase_balance_problems/balance_metrics.png" alt="balance统计"></p>
<h1 id="问题大致流程Trace">问题大致流程Trace</h1><ul>
<li>1)  全量索引M/R  job 失败   </li>
<li>2)  HFile文件不存在  </li>
<li>3)  HFile发生了compaction，文件被代替  </li>
<li>4)  HFile文件数量达到默认数量(3个) 触发Minor compaction  </li>
<li>5)  Memstore被flush，生成了HFile  </li>
<li>6)  HRegion 被close之前会触发flush (包括snapshot，倒到Memory的阀值也会触发)，但close是主要原因。  </li>
<li>7)  Cluster的Balance操作需要 Close &amp; Move HRegion  </li>
<li><p>8)  StochasticLoadBalancer HBase的Balance策略机制类，触发条件为:  </p>
<ul>
<li>a)  达到Balance触发条件 Max Region &gt; AVG(Region数量) +1  OR  Min Region &lt;AVG(Region数量) -1  </li>
<li>b)  调整后的Cost Function值比原来的小（策略更为优秀）<br>附:  CF的值:<br>new ReadRequestCostFunction(conf),<br>new WriteRequestCostFunction(conf),<br>new MemstoreSizeCostFunction(conf),<br>new StoreFileCostFunction(conf)</li>
</ul>
</li>
<li><p>9)  新增的Region数量并不多。不会经常触发balance</p>
</li>
<li>10)  Balance的大部分Region都是旧的Region，并且 无法调整到最优状态，导致不断调整，死循环。如下某一个时刻的Region分布:<br><img src="/img/hbase_balance_problems/regions_info.png" alt="region分布"></li>
</ul>
<p>只要Region数小于73或者大于76，就会满足条件a。<br>并且，如果机会调整之后的cost比较小，就会满足b。</p>
<ul>
<li>11)  调整不到平衡点，可能是因为某些调整失败。</li>
<li>12)  没有加载Phoenix coprocessor的机器如果balance 一些有该coprocessor的Region就会失败。</li>
<li>13)  大规模调整从9/3 18:00开始第一个balance周期。 </li>
<li>14)  9/3 17:57 分执行了truncate的操作。 </li>
</ul>
<h1 id="Balance_Trace">Balance Trace</h1><p>因为日志涉及公司信息，不再给出，大体步骤如下:<br>需要调整的Region号:  XXXXX</p>
<ul>
<li>Step 1: 将该Region从server12迁移到server24 (这台机器已dead)</li>
<li>Step 2: 因为迁移server24失败，所以选择另一台机器进行迁移，选择了server34</li>
<li>Step 3:  server34加载该Region失败，因为coprocessor的原因。</li>
<li>Step 4 : 选择server32迁移，也失败了，最终选择了server19成功</li>
</ul>
<p>Balance的总结:    </p>
<ul>
<li>a) 我们可以看到，Balance的策略是失败的，因为最初是想将这个Region迁移到server34，而最终却是迁移到了server19。  </li>
<li>b) 这样的balance，不仅仅是失败，而且会对server19增加了负担。</li>
<li>c) 从HBase代码我没有看到对这种情况的处理方式。但理论上一次失败是没有关系的，只要第二次Balance成功即可。</li>
<li>d) 但是，某些机器注定无法收容某些Region，所以，注定永远无法调整到一个平衡的balance掉。</li>
</ul>
<p>遗留问题:<br>从日志中有一个问题: server24这台机器在9/3 15:29已经dead，为何balance策略仍然会选择这台机器。是不是又bug…</p>
<hr>
<p>作者：3h-william<br>联系: <a href="https://github.com/3h-william" target="_blank" rel="external">https://github.com/3h-william</a><br>出处：<a href="http://3h-william.github.io" target="_blank" rel="external">http://3h-william.github.io</a>  </p>
<hr>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2015/09/08/hbase_balance_problem/" data-id="cigspks7n000cuogtf6o9wlps" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/hbase/">hbase</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-hbase-bulkload" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2015/08/27/hbase-bulkload/" class="article-date">
  <time datetime="2015-08-27T06:08:11.000Z" itemprop="datePublished">2015-08-27</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2015/08/27/hbase-bulkload/">hbase bulkload 相关整理</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>由于近期Team内有同学用到bulkload,整理了一下过往我用到的bulkload的相关使用经验<br>基于<strong>hbase 0.90</strong>和<strong>0.94</strong>版本做一些简单的分析，可供参考。  </p>
<h2 id="1-_概述"><strong>1. 概述</strong></h2><p><a href="http://hbase.apache.org/book.html#arch.bulk.load" target="_blank" rel="external">官方说明</a><br>[0.94官方说明] (<a href="http://hbase.apache.org/0.94/book/arch.bulk.load.html" target="_blank" rel="external">http://hbase.apache.org/0.94/book/arch.bulk.load.html</a>)  </p>
<p>简单的说，bluk load 是能够批量加载hfile =&gt; hbase的技术。<br>服务端，RegionServer处理客户端的bulkload请求。<br>客户端，向RegionServer发起blukload请求。  </p>
<h3 id="1-1-_案例"><strong>1.1. 案例</strong></h3><p>在2013年的maillog项目中，我们使用了bulk load技术。批量加载了从M/R生成的HFile数据。<br>因为是预先生成的HFile和处理过的，所以是一个HFile对应1个Region</p>
<h4 id="加载数据量:"><strong>加载数据量:</strong></h4><p>HFile(Region) 总数: <strong>100个</strong><br>占用磁盘大小 : <strong>450G</strong> (snappy压缩后<strong>100G</strong>)<br>消耗时间 : 在数据已经存在与HDFS的情况下，每一个HFile加载时间 <strong>小于 1秒</strong>    </p>
<h3 id="1-2_特性"><strong>1.2 特性</strong></h3><p><strong>a)</strong> bulkload 在加载的同时，并不会影响正常读写。(如果写文件造成分区变化，对bulkload有影响)<br><strong>b)</strong> bulkload 的加载是秒级的，在同一个HDFS cluster中，会将数据move。<br><strong>c)</strong> bulkload 也支持从不同的hdfs cluster copy数据(不同版本没有测试过)，同一个版本可行，它会把原来的move操作替换为copy，效率会降低。<br>(<strong>注意</strong>: 如果你的数据没有move而是copy，请check一下hdfs的URI写的是否正确一致)</p>
<h3 id="1-3-_空表和非空表的策略"><strong>1.3. 空表和非空表的策略</strong></h3><h4 id="场景定义">场景定义</h4><p>bulkload 适用于初始化整个HBase表(空表)， 也适用于对于增量数据的导入（非空表）  </p>
<h4 id="策略">策略</h4><p>显然，对于这两种场景的操作手法和策略是不同的。<br>对于任何的bulkload操作策略来说，评判目标应该是一致的: <strong>效率和影响</strong><br><strong>效率</strong> 指的是如何在短时间内将数据load。<br><strong>影响</strong> 指的是如何避免在做bulkload的时候，减少对集群的影响，避免hbase触发split/merge等heavy的操作。    </p>
<h4 id="操作手法一般包括：">操作手法一般包括：</h4><p><strong>a) HFile的预处理</strong>  通过M/R Job或者其他手段预先处理HFile，可以定义HFile大小，splitkey规则，可以在非空表的情况下，极大避免对集群的影响。<br><strong>b) 预分区</strong>  通常来说，对于空表的bulkload应该是不会触发集群的split、merge等操作的。通过预分区可以有效的规避这一点。和Hbase 的split size结合使用。 </p>
<h2 id="2-_Bulkload客户端流程"><strong>2. Bulkload客户端流程</strong></h2><p>主要围绕<strong>LoadIncrementalHFiles</strong> 类来说明基本的流程。<br>可参考我上传的代码，对比以下链接的代码<br>[0.90版本官方提供的LoadIncrementalHFiles类] (<a href="http://trgit2/wz68/importdb_blukload/blob/master/src/main/java/com/newegg/email/importdb/bulkload/tool/LoadIncrementalHFiles.java" target="_blank" rel="external">http://trgit2/wz68/importdb_blukload/blob/master/src/main/java/com/newegg/email/importdb/bulkload/tool/LoadIncrementalHFiles.java</a>)<br>[基于0.90版本修改后支持客户端并发的LoadIncrementalHFilesConcurrency类]  (<a href="http://trgit2/wz68/importdb_blukload/blob/master/src/main/java/com/newegg/email/importdb/bulkload/tool/LoadIncrementalHFilesConcurrency.java" target="_blank" rel="external">http://trgit2/wz68/importdb_blukload/blob/master/src/main/java/com/newegg/email/importdb/bulkload/tool/LoadIncrementalHFilesConcurrency.java</a>)<br>总的来说，这部分代码比较简单，客户端代码很容易读懂。</p>
<h3 id="Step_1-_main_entry">Step 1.  main entry</h3><p>  LoadIncrementalHFiles的entry方法，会接收两个参数，一个是表的名字，另一个是<strong>hfile文件或文件夹路径</strong>(此处可传递文件夹)<br>  这里的LoadIncrementalHFiles是implements于org.apache.hadoop.util.Tool，但是因为这里只是借助于tool这个工具，实质上并没有启动任何M/R job，因此我们看到的只是一个local的app启动。</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">   <span class="comment">// entry 方法</span></span><br><span class="line">   <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> throws Exception </span>&#123;</span><br><span class="line">       <span class="keyword">int</span> ret = ToolRunner.run(<span class="keyword">new</span> LoadIncrementalHFiles(HBaseConfiguration.create()), args);</span><br><span class="line">    System.<span class="built_in">exit</span>(ret);</span><br><span class="line">   &#125;</span><br><span class="line"></span><br><span class="line">   <span class="comment">// tool的默认加载方法run</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">run</span><span class="params">(String[] args)</span> throws Exception </span>&#123;</span><br><span class="line">	<span class="keyword">if</span> (args.length != <span class="number">2</span>) &#123;</span><br><span class="line">		usage();</span><br><span class="line">		<span class="keyword">return</span> -<span class="number">1</span>;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	String dirPath = args[<span class="number">0</span>];      <span class="comment">// ---&gt; 路径(文件、文件夹)</span></span><br><span class="line">	String tableName = args[<span class="number">1</span>];    <span class="comment">// ---&gt; table name</span></span><br><span class="line"></span><br><span class="line">	boolean tableExists = <span class="keyword">this</span>.doesTableExist(tableName);</span><br><span class="line">	<span class="keyword">if</span> (!tableExists)</span><br><span class="line">		<span class="keyword">this</span>.createTable(tableName, dirPath);</span><br><span class="line"></span><br><span class="line">	Path hfofDir = <span class="keyword">new</span> Path(dirPath);</span><br><span class="line">	HTable table = <span class="keyword">new</span> HTable(conf,tableName);</span><br><span class="line"></span><br><span class="line">	doBulkLoad(hfofDir, table);   <span class="comment">// ---&gt; 调用bulkload方法</span></span><br><span class="line">	<span class="keyword">return</span> <span class="number">0</span>;                     <span class="comment">// ---&gt;  Run方法中并没有调用M/R job client，因此只是一个local app，不能算是M/R的job</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="Step_2_-_串行化和并行化_doBulkLoad">Step 2 . 串行化和并行化 doBulkLoad</h3><p>串行化和并行化目前已经在高版本应该解决了。  </p>
<h4 id="a)_0-90版本的串行化:_以下这段是官方提供的代码，我们可以看到tryLoad是循环的。所以无法并发去做。"><strong>a)</strong>  0.90版本的串行化: 以下这段是官方提供的代码，我们可以看到tryLoad是循环的。所以无法并发去做。</h4><figure class="highlight openscad"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">public void doBulkLoad<span class="params">(Path hfofDir, HTable table)</span> throws TableNotFoundException, IOException &#123;</span><br><span class="line">	HConnection conn = table.getConnection<span class="params">()</span>;</span><br><span class="line"></span><br><span class="line">	<span class="keyword">if</span> <span class="params">(!conn.isTableAvailable<span class="params">(table.getTableName<span class="params">()</span>)</span>)</span> &#123;</span><br><span class="line">		throw new TableNotFoundException<span class="params">(<span class="string">"Table "</span> + Bytes.toStringBinary<span class="params">(table.getTableName<span class="params">()</span>)</span> + <span class="string">"is not currently available."</span>)</span>;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	Deque&lt;LoadQueueItem&gt; queue = null;</span><br><span class="line">	try &#123;</span><br><span class="line">		queue = discoverLoadQueue<span class="params">(hfofDir)</span>;</span><br><span class="line">		while <span class="params">(!queue.isEmpty<span class="params">()</span>)</span> &#123;                                <span class="comment">// ----&gt;  串行化消费队列，提交bulkload</span></span><br><span class="line">			LoadQueueItem item = queue.remove<span class="params">()</span>;</span><br><span class="line">			tryLoad<span class="params">(item, conn, table.getTableName<span class="params">()</span>, queue)</span>;</span><br><span class="line">		&#125;</span><br><span class="line">	&#125; finally &#123;</span><br><span class="line">		<span class="keyword">if</span> <span class="params">(queue != null &amp;&amp; !queue.isEmpty<span class="params">()</span>)</span> &#123;</span><br><span class="line">			StringBuilder err = new StringBuilder<span class="params">()</span>;</span><br><span class="line">			err.append<span class="params">(<span class="string">"-------------------------------------------------\n"</span>)</span>;</span><br><span class="line">			err.append<span class="params">(<span class="string">"Bulk load aborted with some files not yet loaded:\n"</span>)</span>;</span><br><span class="line">			err.append<span class="params">(<span class="string">"-------------------------------------------------\n"</span>)</span>;</span><br><span class="line">			<span class="keyword">for</span> <span class="params">(LoadQueueItem q : queue)</span> &#123;</span><br><span class="line">				err.append<span class="params">(<span class="string">"  "</span>)</span>.append<span class="params">(q.hfilePath)</span>.append<span class="params">('\n')</span>;</span><br><span class="line">			&#125;</span><br><span class="line">			LOG.error<span class="params">(err)</span>;</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="b)_基于0-90版本修改的并行化方案，通过构建线程池来完成。"><strong>b)</strong>  基于0.90版本修改的并行化方案，通过构建线程池来完成。</h4><figure class="highlight openscad"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">public boolean doBulkLoad<span class="params">(Path hfofDir, HTable table)</span> throws TableNotFoundException, IOException &#123;</span><br><span class="line">	boolean isSuccess=<span class="literal">false</span>;</span><br><span class="line">	HConnection conn = table.getConnection<span class="params">()</span>;</span><br><span class="line">	</span><br><span class="line">	<span class="keyword">if</span> <span class="params">(!conn.isTableAvailable<span class="params">(table.getTableName<span class="params">()</span>)</span>)</span> &#123;</span><br><span class="line">		throw new TableNotFoundException<span class="params">(<span class="string">"Table "</span> + Bytes.toStringBinary<span class="params">(table.getTableName<span class="params">()</span>)</span> + <span class="string">"is not currently available."</span>)</span>;</span><br><span class="line">	&#125;</span><br><span class="line">	List&lt;Deque&lt;LoadQueueItem&gt;&gt; queueList = null;</span><br><span class="line">	<span class="comment">// Deque&lt;LoadQueueItem&gt; queue = null;</span></span><br><span class="line">	queueList = discoverLoadQueue<span class="params">(hfofDir)</span>;</span><br><span class="line">	int threadNums=conf.getInt<span class="params">(CONCURRENCY_NUMS, <span class="number">1</span>)</span>;</span><br><span class="line">	LOG.info<span class="params">(<span class="string">"*********    thread nums = "</span>+threadNums)</span>;</span><br><span class="line">	ExecutorService executorService = Executors.newFixedThreadPool<span class="params">(threadNums)</span>;                <span class="comment">//  -----&gt; 并行化解决方案</span></span><br><span class="line">	List&lt;Future&lt;Boolean&gt;&gt; resultList = new ArrayList&lt;Future&lt;Boolean&gt;&gt;<span class="params">()</span>;</span><br><span class="line">	try&#123;</span><br><span class="line">		<span class="keyword">for</span> <span class="params">(Deque&lt;LoadQueueItem&gt; queue : queueList)</span> &#123;</span><br><span class="line">			Future&lt;Boolean&gt; future =executorService.submit<span class="params">(new LoadThread<span class="params">(queue, conn, table.getTableName<span class="params">()</span>)</span>)</span>;</span><br><span class="line">			resultList.add<span class="params">(future)</span>;</span><br><span class="line">		&#125;</span><br><span class="line">		<span class="keyword">for</span> <span class="params">(Future&lt;Boolean&gt; result : resultList)</span> &#123;</span><br><span class="line">			<span class="keyword">if</span> <span class="params">(!result.get<span class="params">(<span class="number">100</span>, TimeUnit.MINUTES)</span>)</span> &#123;</span><br><span class="line">				LOG.error<span class="params">(<span class="string">"validatorThread return false"</span>)</span>;</span><br><span class="line">				return <span class="literal">false</span>;</span><br><span class="line">			&#125;</span><br><span class="line">		&#125;</span><br><span class="line">		isSuccess = <span class="literal">true</span>;</span><br><span class="line">	&#125;catch<span class="params">(Exception e)</span>&#123;</span><br><span class="line">		LOG.error<span class="params">(e)</span>;</span><br><span class="line">	&#125;finally&#123;</span><br><span class="line">		executorService.shutdown<span class="params">()</span>;</span><br><span class="line">		return isSuccess;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="c)_官方0-94的版本的代码片段，已经提供了并行化，并且大幅修改了代码，开启并行化，需要手动设置参数:_hbase-loadincremental-threads-max，默认值是机器的cpu_cores相关"><strong>c)</strong> 官方0.94的版本的代码片段，已经提供了并行化，并且大幅修改了代码，开启并行化，需要手动设置参数: <strong>hbase.loadincremental.threads.max</strong>，默认值是机器的cpu cores相关</h4><figure class="highlight openscad"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">public void doBulkLoad<span class="params">(Path hfofDir, final HTable table)</span></span><br><span class="line">   throws TableNotFoundException, IOException</span><br><span class="line"> &#123;</span><br><span class="line">   final HConnection conn = table.getConnection<span class="params">()</span>;</span><br><span class="line"></span><br><span class="line">   <span class="keyword">if</span> <span class="params">(!conn.isTableAvailable<span class="params">(table.getTableName<span class="params">()</span>)</span>)</span> &#123;</span><br><span class="line">     throw new TableNotFoundException<span class="params">(<span class="string">"Table "</span> +</span><br><span class="line">         Bytes.toStringBinary<span class="params">(table.getTableName<span class="params">()</span>)</span> +</span><br><span class="line">         <span class="string">"is not currently available."</span>)</span>;</span><br><span class="line">   &#125;</span><br><span class="line"></span><br><span class="line">   <span class="comment">// initialize thread pools                                                        ----&gt;  并行化处理加载HFile</span></span><br><span class="line">   int nrThreads = cfg.getInt<span class="params">(<span class="string">"hbase.loadincremental.threads.max"</span>,</span><br><span class="line">       Runtime.getRuntime<span class="params">()</span>.availableProcessors<span class="params">()</span>)</span>;</span><br><span class="line">   ThreadFactoryBuilder builder = new ThreadFactoryBuilder<span class="params">()</span>;</span><br><span class="line">   builder.setNameFormat<span class="params">(<span class="string">"LoadIncrementalHFiles-%1$d"</span>)</span>;</span><br><span class="line">   ExecutorService pool = new ThreadPoolExecutor<span class="params">(nrThreads, nrThreads,</span><br><span class="line">       <span class="number">60</span>, TimeUnit.SECONDS,</span><br><span class="line">       new LinkedBlockingQueue&lt;Runnable&gt;<span class="params">()</span>,</span><br><span class="line">       builder.build<span class="params">()</span>)</span>;</span><br><span class="line">   <span class="params">(<span class="params">(ThreadPoolExecutor)</span>pool)</span>.allowCoreThreadTimeOut<span class="params">(<span class="literal">true</span>)</span>;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">   ......</span><br></pre></td></tr></table></figure>
<h3 id="Step_3-_查看hfile边界，以决定是否需要split">Step 3. 查看hfile边界，以决定是否需要split</h3><p>这个地方就是是否会在客户端发生split的原因<br>直接参考0.94的代码<br><figure class="highlight openscad"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span><br><span class="line">   * Attempt to assign the given load queue item into its target region group.</span><br><span class="line">   * If the hfile boundary no longer fits into a region, physically splits</span><br><span class="line">   * the hfile such that the new bottom half will fit and returns the list of</span><br><span class="line">   * LQI's corresponding to the resultant hfiles.</span><br><span class="line">   *</span><br><span class="line">   * protected for testing</span><br><span class="line">   */</span></span><br><span class="line">  protected List&lt;LoadQueueItem&gt; groupOrSplit<span class="params">(Multimap&lt;ByteBuffer, LoadQueueItem&gt; regionGroups,</span><br><span class="line">      final LoadQueueItem item, final HTable table,</span><br><span class="line">      final Pair&lt;byte[][], byte[][]&gt; startEndKeys)</span></span><br><span class="line">      throws IOException &#123;</span><br><span class="line">    final Path hfilePath = item.hfilePath;</span><br><span class="line">    final FileSystem fs = hfilePath.getFileSystem<span class="params">(getConf<span class="params">()</span>)</span>;</span><br><span class="line">    HFile.Reader hfr = HFile.createReader<span class="params">(fs, hfilePath,</span><br><span class="line">        new CacheConfig<span class="params">(getConf<span class="params">()</span>)</span>)</span>;</span><br><span class="line">    final byte[] first, last;</span><br><span class="line">    try &#123;</span><br><span class="line">      hfr.loadFileInfo<span class="params">()</span>;</span><br><span class="line">      first = hfr.getFirstRowKey<span class="params">()</span>;</span><br><span class="line">      last = hfr.getLastRowKey<span class="params">()</span>;</span><br><span class="line">    &#125;  finally &#123;</span><br><span class="line">      hfr.close<span class="params">()</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    LOG.info<span class="params">(<span class="string">"Trying to load hfile="</span> + hfilePath +                       ----&gt; 从hfile中得到first 和 last的信息</span><br><span class="line">        <span class="string">" first="</span> + Bytes.toStringBinary<span class="params">(first)</span> +</span><br><span class="line">        <span class="string">" last="</span>  + Bytes.toStringBinary<span class="params">(last)</span>)</span>;</span><br><span class="line">    <span class="keyword">if</span> <span class="params">(first == null || last == null)</span> &#123;</span><br><span class="line">      assert first == null &amp;&amp; last == null;</span><br><span class="line">      <span class="comment">// TODO what if this is due to a bad HFile?</span></span><br><span class="line">      LOG.info<span class="params">(<span class="string">"hfile "</span> + hfilePath + <span class="string">" has no entries, skipping"</span>)</span>;</span><br><span class="line">      return null;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> <span class="params">(Bytes.compareTo<span class="params">(first, last)</span> &gt; <span class="number">0</span>)</span> &#123;                              </span><br><span class="line">      throw new IllegalArgumentException<span class="params">(</span><br><span class="line">      <span class="string">"Invalid range: "</span> + Bytes.toStringBinary<span class="params">(first)</span> +</span><br><span class="line">      <span class="string">" &gt; "</span> + Bytes.toStringBinary<span class="params">(last)</span>)</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    int idx = Arrays.binarySearch<span class="params">(startEndKeys.getFirst<span class="params">()</span>, first,        ---&gt;  从整个集群的statEndKeys的多列中搜索是否满足该HFile的Region</span><br><span class="line">        Bytes.BYTES_COMPARATOR)</span>;</span><br><span class="line">    <span class="keyword">if</span> <span class="params">(idx &lt; <span class="number">0</span>)</span> &#123;</span><br><span class="line">      <span class="comment">// not on boundary, returns -(insertion index).  Calculate region it</span></span><br><span class="line">      <span class="comment">// would be in.</span></span><br><span class="line">      idx = -<span class="params">(idx + <span class="number">1</span>)</span> - <span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    final int indexForCallable = idx;</span><br><span class="line">    boolean lastKeyInRange =</span><br><span class="line">      Bytes.compareTo<span class="params">(last, startEndKeys.getSecond<span class="params">()</span>[idx])</span> &lt; <span class="number">0</span> ||</span><br><span class="line">      Bytes.equals<span class="params">(startEndKeys.getSecond<span class="params">()</span>[idx], HConstants.EMPTY_BYTE_ARRAY)</span>;</span><br><span class="line">    <span class="keyword">if</span> <span class="params">(!lastKeyInRange)</span> &#123;</span><br><span class="line">      List&lt;LoadQueueItem&gt; lqis = splitStoreFile<span class="params">(item, table,             ---&gt;  如果不满足，就会触发客户端split，调用splitStoreFile方法这里就时效率慢的原因！！！！</span><br><span class="line">          startEndKeys.getFirst<span class="params">()</span>[indexForCallable],</span><br><span class="line">          startEndKeys.getSecond<span class="params">()</span>[indexForCallable])</span>;</span><br><span class="line">      return lqis;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// group regions.</span></span><br><span class="line">    regionGroups.put<span class="params">(ByteBuffer.wrap<span class="params">(startEndKeys.getFirst<span class="params">()</span>[idx])</span>, item)</span>;         ---&gt; 如果HFile满足区间，或者是已经将不满足区间的HFile切分满足，那么就加到RegionGroup中，等待加载.</span><br><span class="line">    return null;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure></p>
<h3 id="Step_4-_客户端split的两个方法">Step 4. 客户端split的两个方法</h3><figure class="highlight processing"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 决定split的范围，数量，分区</span></span><br><span class="line">  <span class="keyword">protected</span> List&lt;LoadQueueItem&gt; splitStoreFile(<span class="keyword">final</span> LoadQueueItem item,</span><br><span class="line">      <span class="keyword">final</span> HTable table, <span class="built_in">byte</span>[] startKey,</span><br><span class="line">      <span class="built_in">byte</span>[] splitKey) <span class="keyword">throws</span> IOException &#123;</span><br><span class="line">    <span class="keyword">final</span> Path hfilePath = item.hfilePath;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// We use a '_' prefix which is ignored when walking directory trees</span></span><br><span class="line">    <span class="comment">// above.</span></span><br><span class="line">    <span class="keyword">final</span> Path tmpDir = <span class="keyword">new</span> Path(item.hfilePath.getParent(), <span class="string">"_tmp"</span>);</span><br><span class="line"></span><br><span class="line">    LOG.info(<span class="string">"HFile at "</span> + hfilePath + <span class="string">" no longer fits inside a single "</span> +</span><br><span class="line">    <span class="string">"region. Splitting..."</span>);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">String</span> uniqueName = getUniqueName(table.getTableName());</span><br><span class="line">    HColumnDescriptor familyDesc = table.getTableDescriptor().getFamily(item.family);</span><br><span class="line">    Path botOut = <span class="keyword">new</span> Path(tmpDir, uniqueName + <span class="string">".bottom"</span>);</span><br><span class="line">    Path topOut = <span class="keyword">new</span> Path(tmpDir, uniqueName + <span class="string">".top"</span>);</span><br><span class="line">    splitStoreFile(getConf(), hfilePath, familyDesc, splitKey,</span><br><span class="line">        botOut, topOut);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Add these back at the *front* of the queue, so there's a lower</span></span><br><span class="line">    <span class="comment">// chance that the region will just split again before we get there.</span></span><br><span class="line">    List&lt;LoadQueueItem&gt; lqis = <span class="keyword">new</span> ArrayList&lt;LoadQueueItem&gt;(<span class="number">2</span>);</span><br><span class="line">    lqis.<span class="built_in">add</span>(<span class="keyword">new</span> LoadQueueItem(item.family, botOut));</span><br><span class="line">    lqis.<span class="built_in">add</span>(<span class="keyword">new</span> LoadQueueItem(item.family, topOut));</span><br><span class="line"></span><br><span class="line">    LOG.info(<span class="string">"Successfully split into new HFiles "</span> + botOut + <span class="string">" and "</span> + topOut);      <span class="comment">// 如果split成功，我们会看到这些日志...</span></span><br><span class="line">    <span class="keyword">return</span> lqis;</span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line">  </span><br><span class="line">  <span class="comment">// 真正调用执行split,copy....</span></span><br><span class="line">  </span><br><span class="line">    <span class="comment">/**</span><br><span class="line">   * Split a storefile into a top and bottom half, maintaining</span><br><span class="line">   * the metadata, recreating bloom filters, etc.</span><br><span class="line">   */</span></span><br><span class="line">  <span class="keyword">static</span> <span class="keyword">void</span> splitStoreFile(</span><br><span class="line">      Configuration conf, Path inFile,</span><br><span class="line">      HColumnDescriptor familyDesc, <span class="built_in">byte</span>[] splitKey,</span><br><span class="line">      Path bottomOut, Path topOut) <span class="keyword">throws</span> IOException</span><br><span class="line">  &#123;</span><br><span class="line">    <span class="comment">// Open reader with no block cache, and not in-memory</span></span><br><span class="line">    Reference topReference = <span class="keyword">new</span> Reference(splitKey, Range.top);</span><br><span class="line">    Reference bottomReference = <span class="keyword">new</span> Reference(splitKey, Range.bottom);</span><br><span class="line"></span><br><span class="line">    copyHFileHalf(conf, inFile, topOut, topReference, familyDesc);                    -----&gt; 客户端<span class="built_in">split</span>的真正<span class="built_in">copy</span>的方法</span><br><span class="line">    copyHFileHalf(conf, inFile, bottomOut, bottomReference, familyDesc); </span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
<h2 id="3-_Q&amp;A"><strong>3. Q&amp;A</strong></h2><h3 id="1)_Q:_目前的工具是否是并行化加载?"><strong>1)  Q:  目前的工具是否是并行化加载?</strong></h3><pre><code>通过上面的分析，我们可以看到，从0.94开始就已经是有并行化处理了，因此我们不需要再像我之前的那种方式去做了。只要定义好自己的线程池大小就ok.   
但我们也需要注意的是，由于<span class="keyword">*</span><span class="keyword">*</span>LoadIncrementalHFiles<span class="keyword">*</span><span class="keyword">*</span> 工具并不是M/R job，所以不要指望能直接通过M/R来提高效率。  
</code></pre><h3 id="2)_Q:_如果避免split?"><strong>2)  Q:  如果避免split?</strong></h3><pre><code><span class="built_in">split</span>有两个定义。一个是客户端的<span class="built_in">split</span>，另一个是Region Server的<span class="built_in">split</span>。  
在空表情况下，通过预分区的方式，只要<span class="variable">key</span>分的准确，就不会触发客户端的<span class="built_in">split</span>。结合hbase 原本的 <span class="built_in">split</span> <span class="built_in">size</span> 参数，使HFile对应的HRegion大小合适，不会造成RSr的<span class="built_in">split</span>.
</code></pre><h3 id="3)_Q:_对空表的bulkload的过程中，可否正常读写?"><strong>3)  Q:  对空表的bulkload的过程中，可否正常读写?</strong></h3><pre><code>读肯定是没有问题的，写的话，如果数据量不大，不触发Region的<span class="built_in">split</span>，则也没有关系。  
</code></pre><h3 id="4)_Q:_为什么bulkload的过程中，在数据已经准备的情况下，效率那么快，秒级别加载，并且不会disable_Region?"><strong>4)  Q:  为什么bulkload的过程中，在数据已经准备的情况下，效率那么快，秒级别加载，并且不会disable Region?</strong></h3><pre><code>前面只分析了Bulkload的客户端代码，如果去翻阅一下服务端代码我们可以知道。对于<span class="keyword">RS</span>来说，bulkload就两件事情，第一件是mv文件，第二件是在<span class="keyword">RS</span>的memory里加上新HFile的meta信息。  
</code></pre><h3 id="5)_Q:_对于非空表的增量数据bulkload，基本策略有哪些?"><strong>5)  Q:  对于非空表的增量数据bulkload，基本策略有哪些?</strong></h3><pre><code>参考步骤如下:  
a)  对新增HFiles大小做预估计，如果当前<span class="keyword">cluster</span>可以容纳，则不需要预分区，否则，可以先预分区。  
b)  通过<span class="keyword">M</span>/R对HFile做符合当前集群分区情况进行切分。  
c)  加载。  
</code></pre><p>对此，官方设计中有一段说明:<br>If the region boundaries have changed during the course of bulk load preparation, or between the preparation and completion steps, the completebulkloads utility will automatically split the data files into pieces corresponding to the new boundaries. This process is not optimally efficient, so users should take care to minimize the delay between preparing a bulk load and importing it into the cluster, especially if other clients are simultaneously loading data through other means.</p>
<p>大致意思是，即使客户端工具能够完成split的工作，但并不是最有效的方式，用户应该注意边界问题。</p>
<hr>
<p>作者：3h-william<br>联系: <a href="https://github.com/3h-william" target="_blank" rel="external">https://github.com/3h-william</a><br>出处：<a href="http://3h-william.github.io" target="_blank" rel="external">http://3h-william.github.io</a>  </p>
<hr>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2015/08/27/hbase-bulkload/" data-id="cigspks6z0000uogtt9katlwb" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/hbase/">hbase</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-hello-world" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2015/08/27/hello-world/" class="article-date">
  <time datetime="2015-08-27T06:08:11.000Z" itemprop="datePublished">2015-08-27</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2015/08/27/hello-world/">Hello World , Hello PPG , Hello Sunshine</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>You are always my sunshine…</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2015/08/27/hello-world/" data-id="cigspks7l000buogt133kcsgz" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
  
</section>
        
          <aside id="sidebar">
  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list"><li class="tag-list-item"><a class="tag-list-link" href="/tags/hbase/">hbase</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/hive/">hive</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/sqoop/">sqoop</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/zookeeper/">zookeeper</a><span class="tag-list-count">1</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/hbase/" style="font-size: 20px;">hbase</a> <a href="/tags/hive/" style="font-size: 10px;">hive</a> <a href="/tags/sqoop/" style="font-size: 10px;">sqoop</a> <a href="/tags/zookeeper/" style="font-size: 10px;">zookeeper</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/11/">November 2015</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/09/">September 2015</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/08/">August 2015</a><span class="archive-list-count">2</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recents</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2015/11/10/zookeeper_stuck_dns_network/">使用Zookeeper过程中遇到客户端stuck的问题和解决建议</a>
          </li>
        
          <li>
            <a href="/2015/09/11/sqoop_orc/">使用sqoop生成ORC格式的文件，并给Hive加载</a>
          </li>
        
          <li>
            <a href="/2015/09/08/hbase_balance_problem/">HBase Coprocessor问题引起异常Balance</a>
          </li>
        
          <li>
            <a href="/2015/08/27/hbase-bulkload/">hbase bulkload 相关整理</a>
          </li>
        
          <li>
            <a href="/2015/08/27/hello-world/">Hello World , Hello PPG , Hello Sunshine</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2015 3h_william<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css" type="text/css">
  <script src="/fancybox/jquery.fancybox.pack.js" type="text/javascript"></script>


<script src="/js/script.js" type="text/javascript"></script>

  </div>
</body>
</html>