<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>3h_William</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description">
<meta property="og:type" content="website">
<meta property="og:title" content="3h_William">
<meta property="og:url" content="http://yoursite.com/index.html">
<meta property="og:site_name" content="3h_William">
<meta property="og:description">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="3h_William">
<meta name="twitter:description">
  
    <link rel="alternative" href="/atom.xml" title="3h_William" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  <link rel="stylesheet" href="/css/style.css" type="text/css">
  

</head>
<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">3h_William</a>
      </h1>
      
        <h2 id="subtitle-wrap">
          <a href="/" id="subtitle">Hello Sunshine</a>
        </h2>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" results="0" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://yoursite.com"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main">
  
    <article id="post-zookeeper_stuck_dns_network" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2015/11/10/zookeeper_stuck_dns_network/" class="article-date">
  <time datetime="2015-11-10T01:18:11.000Z" itemprop="datePublished">2015-11-10</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2015/11/10/zookeeper_stuck_dns_network/">使用Zookeeper过程中遇到客户端stuck的问题和解决建议</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="背景">背景</h1><p>最近在查询一个和zookeeper相关的case。追查结果是一个称之为”Reverse DNS lookup”的问题。<br>可以称之为:反向DNS查找。 <a href="https://en.wikipedia.org/wiki/Reverse_DNS_lookup" target="_blank" rel="external">https://en.wikipedia.org/wiki/Reverse_DNS_lookup</a></p>
<h1 id="隐患">隐患</h1><p>zookeeper 3.5以下的版本，会在客户端并发链接的时候，有一定几率发生stuck，随着并发连接的增多，发生概率越大。尤其是第一次创建连接的时候。<br>(因为zookeeper的client版本和server的版本兼容性，可以先升级客户端，客户端版本&gt;服务端版本，是支持的)</p>
<p>原因:<br>1)   归根结底，是因为DNS解析的问题。 可参考:<br><a href="https://issues.apache.org/jira/browse/ZOOKEEPER-1666" target="_blank" rel="external">issue 1666</a>   </p>
<p>2)   1666 issue在3.4.6已经fix，但是和它相关的另一个issue，只有在3.5才fix<br><a href="https://issues.apache.org/jira/browse/ZOOKEEPER-1891" target="_blank" rel="external">issue 1891</a>  </p>
<h1 id="案例与解释">案例与解释</h1><p>简单的来说，问题出在 “通过IP找HostName”，上， 例如: 通过”10.x.x.x”找到”hadoopXXX” 这个方法上。<br>在计算机网络中，我们可以用ip也可以用HostName来表明一台唯一的机器，我们也知道有一个叫做DNS的服务来帮助我们将IP转化为hostName. <strong>但是，这一个转换是有开销的</strong>。<br>是需要通过上层的DNS服务器或者IP机器本身的网卡来达到映射。<br>至于细节，可以参考wiki  </p>
<p>接下来，我们可以本地来模拟这个过程:<br>我们用Java可以启动以下代码，其中，注释的两行是两种构建方式。  </p>
<figure class="highlight lisp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">        for<span class="list">(<span class="keyword">int</span> i=0<span class="comment">;i&lt;100;i++)&#123;</span></span><br><span class="line">            new Thread<span class="list">(<span class="keyword">new</span> Runnable<span class="list">()</span> &#123;</span><br><span class="line">                @Override</span><br><span class="line">                public void run<span class="list">()</span> &#123;</span><br><span class="line">//   <span class="number">1</span>                 InetSocketAddress ia = new InetSocketAddress<span class="list">( <span class="string">"10.x.x.x"</span>,<span class="number">2181</span>)</span><span class="comment">;;</span></span><br><span class="line">//   <span class="number">2</span>                 InetSocketAddress ia = new InetSocketAddress<span class="list">( <span class="string">"hadoopXXX/10.x.x.x"</span>,<span class="number">2181</span>)</span><span class="comment">;</span></span><br><span class="line">                    System.out.println<span class="list">(<span class="keyword">ia</span>.getHostName<span class="list">()</span>)</span><span class="comment">;</span></span><br><span class="line">                &#125;</span><br><span class="line">            &#125;)</span>.start<span class="list">()</span><span class="comment">;</span></span><br><span class="line">        &#125;</span></span><br></pre></td></tr></table></figure>
<ul>
<li>当使用注释第一行的时候，线程是有很大的几率发生stuck.  </li>
<li>当使用注释第二行的时候，是不会有这个问题。<br>而不凑巧对的是，zookeeper3.5以下的版本中，是存在第一行注释的代码的。因此，问题就产生了。  </li>
</ul>
<hr>
<p>作者：3h-william<br>联系: <a href="https://github.com/3h-william" target="_blank" rel="external">https://github.com/3h-william</a><br>出处：<a href="http://3h-william.github.io" target="_blank" rel="external">http://3h-william.github.io</a>  </p>
<hr>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2015/11/10/zookeeper_stuck_dns_network/" data-id="cigx3wnt50003ikgt2oi0iust" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/zookeeper/">zookeeper</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-als_report" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2015/10/05/als_report/" class="article-date">
  <time datetime="2015-10-05T06:08:11.000Z" itemprop="datePublished">2015-10-05</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2015/10/05/als_report/">推荐算法与协同过滤</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="(注:_只是个人半年前的一些研究总结，在此仅此记录。)">(注: 只是个人半年前的一些研究总结，在此仅此记录。)</h1><h1 id="1_背景资料：">1    背景资料：</h1><p>推荐系统基于两种策略：  </p>
<blockquote>
<ul>
<li>a)    基于内容的过滤(Content-based filtering)   </li>
<li>b)    协同过滤 (Collaborative filtering) (CF)<br>模型：  </li>
<li>a)    内容过滤需要根据内容创建模板，诸如用物品的title，名字。等等。<br>当然也可以根据用户建立模板。用户的画像等等。  </li>
<li>b)    基于协同过滤。会从历史记录中，找出item-User之间的关系，并建立相关模型。<br>差异：<br>从某些场景来看，内容过滤更为优秀，比如一个新用户(没有任何的历史)。或是一个新商品，没有任何的关联数据。这个时候基于Content-Based的过滤，显得更恰当。<br>然而，在大多数的场景下，协同过滤会变得更为优秀。原因在于，他更充分的考虑到User-Item之间的关系。当然，该方法也使用的更为广泛。</li>
</ul>
</blockquote>
<h1 id="2_协同过滤的集中分类">2 协同过滤的集中分类</h1><h2 id="2-1_基于物品的协同过滤">2.1 基于物品的协同过滤</h2><p>构建  Item -&gt; User的向量: V(I-&gt;U)<br>      计算 各个Verctor之间的相似度<br>直观理解: 把和你之前喜欢的物品近似的物品推荐给你  </p>
<p><img src="/img/als_report/p1.jpg" alt=""></p>
<h2 id="2-2_基于用户的的协同过滤">2.2 基于用户的的协同过滤</h2><p>构建  User -&gt; Item的向量: V(U-&gt;I)<br>      计算 各个Verctor之间的相似度<br>直观理解：把就是把与你有相同爱好的用户所喜欢的物品,推荐给你<br><img src="/img/als_report/p2.jpg" alt="">  </p>
<h2 id="2-3_比Item-CF和User-CF">2.3 比Item-CF和User-CF</h2><p>ItemCF: 多样性不足 (覆盖率)<br>UserItem: 长尾性不足。<br>综合：兴趣点不同。</p>
<h2 id="2-4_于评分的协同过滤_(Slope_One)">2.4 于评分的协同过滤 (Slope One)</h2><p><img src="/img/als_report/p3.jpg" alt="">  </p>
<h1 id="3_SVD与协同过滤">3  SVD与协同过滤</h1><h2 id="3-1_传统的SVD">3.1  传统的SVD</h2><p>SVD (singular value decomposition) 奇异值分解。<br><img src="/img/als_report/p4.jpg" alt=""><br><img src="/img/als_report/a.jpg" alt=""></p>
<p>低阶近似：</p>
<ol>
<li>给定一个矩阵C，对其奇异值分解：  <img src="/img/als_report/a.jpg" alt=""></li>
<li>构造<img src="/img/als_report/a2.jpg" alt="">，它是将<img src="/img/als_report/a3.jpg" alt="">的第k+1行至M行设为零，也就是把<img src="/img/als_report/a3.jpg" alt="">的最小的r-k个(the r-k smallest)奇异值设为零。  </li>
<li>计算 <img src="/img/als_report/a4.jpg" alt=""><br>特征值数值的大小对矩阵-向量相乘影响的大小成正比，而奇异值和特征值也是正比关系，因此这里选取数值最小的r-k个特征值设为零合乎情理，即我们所希望的C-Ck尽可能的小</li>
</ol>
<h2 id="3-2_传统的SVD能否应用于协同过滤（基于评分）的模型?">3.2  传统的SVD能否应用于协同过滤（基于评分）的模型?</h2><p>首先有几个问题 : </p>
<blockquote>
<ul>
<li>1．    传统的SVD对于高维稀疏矩阵实惠丢失一些特征信息。</li>
<li>2．    当矩阵不完全时，SVD是无法定义的。 （完全我的理解是有些值是没有定义 ，缺省，用latent factor 可以解）</li>
<li>3．    如果只对已知的信息求解，会非常容易过度拟合。 (我的理解:没有加入Lambda*规则化)<br>另外，要计算SVD的特征值，对于高维数据来说，也是代价巨大的。  </li>
</ul>
</blockquote>
<h2 id="3-3_SVD因式分解，实现协同过滤">3.3  SVD因式分解，实现协同过滤</h2><p><img src="/img/als_report/p5.jpg" alt=""> </p>
<p><img src="/img/als_report/p6.jpg" alt=""> </p>
<blockquote>
<ul>
<li><h3 id="公式1_:_因式分解(plain)">公式1 :  因式分解(plain)</h3></li>
</ul>
</blockquote>
<p><img src="/img/als_report/f1.jpg" alt=""><br>设定有那么一个转换，能够将已知的评分Matrix分解成两个Factor矩阵相乘。一个和Item相关，一个和User相关</p>
<blockquote>
<ul>
<li><h3 id="公式2:_规则化*Lambda_(L2)">公式2:  规则化*Lambda (L2)</h3><p>规则化的目的，是防止过度拟合。<br>其中Lambda太大，容易低拟合。Lambda太小，容易过拟合。<br>（这里的Lambda在Spark中需要根据实际的数据样本分布，维度的大小调整）<br>当然，这里用的是L2拟合。（区别于L1）,L2是二次函数拟合。<br>(L2拟合的目的是使得W的元素很小，接近于0，但不等于0，会使得模型简单。这样使得个别值对全局的印象降低，越简单的模型越不容易产生过拟合)<br><img src="/img/als_report/f2.jpg" alt="">  </p>
</li>
<li><h3 id="公式3_基线公式(偏好)">公式3  基线公式(偏好)</h3><p><img src="/img/als_report/f3.jpg" alt=""><br>其中：<br>U是所有投票的均值<br>bu是用户打分相对于均值的偏差。<br>bi 是该item被打分相对于均值的偏差<br>(该公式是ALS算法的最原始形态)  </p>
</li>
<li><h3 id="公式4_单个打分">公式4   单个打分</h3><p><img src="/img/als_report/f4.jpg" alt=""><br>该公式是由 (公式1 )  + (公式3) 推得<br>单条记录的打分  </p>
</li>
<li><h3 id="公式5_整体打分求最小化">公式5   整体打分求最小化</h3></li>
</ul>
</blockquote>
<p><img src="/img/als_report/f5.jpg" alt=""><br>由以上同时推得。</p>
<blockquote>
<ul>
<li><h3 id="公式6_外部数据">公式6   外部数据</h3>为了避免冷启动问题，需要加入外部数据。否则会导致新用户的数据非常少。<br>那么外部数据时什么？ 是基于该用户的一些其他特征（画像）<br><img src="/img/als_report/f6.jpg" alt=""> </li>
</ul>
<ul>
<li><h3 id="公式7_动态时间">公式7   动态时间</h3><p><img src="/img/als_report/f7.jpg" alt=""><br>将时间作为权重考虑进去。<br>目的是：用户的倾向会随着时间而发生变化，使得更好的捕捉最新的趋势。  </p>
</li>
<li><h3 id="公式8_自信度（Implicit_,_latent_factor）">公式8  自信度（Implicit , latent factor）</h3><p><img src="/img/als_report/f8.jpg" alt=""><br>该公式和之后Spark中使用trainImplict的理论基本一致。<br>输入矩阵是不同的Confidence Levels</p>
</li>
<li><h3 id="几种优化结果：">几种优化结果：</h3><p><img src="/img/als_report/p7.jpg" alt="">  </p>
</li>
</ul>
</blockquote>
<h1 id="4_Implict_Feedback">4 Implict Feedback</h1><p>显然，如果用户对于物品已经有的评分，这些数据称之为explicit。<br>但是，有些数据，诸如物品的点击数等，这些并不能直接反应用户对该物品的评分。只是表明了一种倾向，但是，究竟是正向的倾向，还是负向的倾向，是不确定的。  </p>
<p>Implicit feedback 的数据和 explicit data的数据是非常不同的:    </p>
<blockquote>
<ul>
<li>a)     矩阵是完整定义了的。 （explicit的数据会有Miss）</li>
<li>b)     没有负向的反馈。 (没有明显的倾向)</li>
<li>c)     在0值的时候，是没有交互定义的。</li>
<li>d)     对于这部分数据，必然没有十足的把握（相对于显示的评分，信心不足）</li>
<li>e)     当然，用标准的SVD显然是无法求得这种关系的。</li>
</ul>
</blockquote>
<p>以下几张图可以一目了然其计算法则：<br><img src="/img/als_report/p8.jpg" alt="">  </p>
<p><img src="/img/als_report/p9.jpg" alt="">  </p>
<p><img src="/img/als_report/p10.jpg" alt="">  </p>
<p>但，Collaborative Filtering for Implicit Feedback Dataset  这篇论文暂时没有搜索到原文。<br>只能通过代码来分析其计算公式。 </p>
<figure class="highlight lisp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">val confidence = <span class="number">1</span> + alpha * abs<span class="list">(<span class="keyword">rs</span><span class="list">(<span class="keyword">i</span>)</span>)</span></span><br></pre></td></tr></table></figure>
<p>所以，在Spark中，参数为alpha的值，是用来调节自信率的。</p>
<h1 id="5_ALS和SGD">5 ALS和SGD</h1><p>即便我们已经有了上述的许多公式，我们如何求解？</p>
<h2 id="5-1_随机梯度下降(SGD)">5.1   随机梯度下降(SGD)</h2><p>Stochastic Gradient Descent  </p>
<p>在数学上，我们知道。 一个符合convex的函数是可以求得最小值的。<br>这样一来，我们就有了梯度下降的求解方法，其梯度下降的速度，取决于convex函数的凸性强不强。  </p>
<p><img src="/img/als_report/p11.jpg" alt=""><br>这就是梯度下降的迭代公式，当然，该算法已经应用于Mahout中实现。  </p>
<h2 id="5-2_ALS算法">5.2  ALS算法</h2><p>Alternating Least Squares (交替最小方差算法)<br>该算法有两个优点:  </p>
<blockquote>
<ul>
<li>1)    ALS更容易并行化。  </li>
<li>2)    ALS对处理隐式数据更方便。   </li>
</ul>
</blockquote>
<p>根据5.1的公式中：<br><img src="/img/als_report/p12.jpg" alt=""></p>
<p>这两共公式各自有两个变量，我们可以固定其中一个变量，这样使得其编程一个二次函数，能够被最优化求解。<br>而ALS就是不断的交替固定Pu和Qi，然后求解另一个变量的最优值。<br>当然，通常来说,SGD会比ALS更加简单和快速，但是ALS的并行性比较好。如果矩阵是稀疏矩阵，那么ALS更有优势  </p>
<h1 id="6_算法的检验">6 算法的检验</h1><p>RMSE/MSE/MAPK<br>待续….</p>
<h1 id="7_Reference">7 Reference</h1><p>spark,als-ws 基础<br><a href="http://www2.research.att.com/~volinsky/papers/ieeecomputer.pdf" target="_blank" rel="external">http://www2.research.att.com/~volinsky/papers/ieeecomputer.pdf</a></p>
<p>协同过滤部分:<br><a href="http://www.ibm.com/developerworks/cn/web/1103_zhaoct_recommstudy2/index.html" target="_blank" rel="external">http://www.ibm.com/developerworks/cn/web/1103_zhaoct_recommstudy2/index.html</a><br><a href="http://www.52ml.net/297.html" target="_blank" rel="external">http://www.52ml.net/297.html</a></p>
<p>svd 部分：<br><a href="http://blog.csdn.net/wangran51/article/details/7408414" target="_blank" rel="external">http://blog.csdn.net/wangran51/article/details/7408414</a><br><a href="http://blog.csdn.net/qianhen123/article/details/40077873" target="_blank" rel="external">http://blog.csdn.net/qianhen123/article/details/40077873</a></p>
<p>规则化:<br><a href="http://blog.csdn.net/u012162613/article/details/44261657" target="_blank" rel="external">http://blog.csdn.net/u012162613/article/details/44261657</a></p>
<p>Latent-factor-models<br><a href="http://www.slideshare.net/sscdotopen/latent-factor-models-for-collaborative-filtering" target="_blank" rel="external">http://www.slideshare.net/sscdotopen/latent-factor-models-for-collaborative-filtering</a></p>
<hr>
<p>作者：3h-william<br>联系: <a href="https://github.com/3h-william" target="_blank" rel="external">https://github.com/3h-william</a><br>出处：<a href="http://3h-william.github.io" target="_blank" rel="external">http://3h-william.github.io</a>  </p>
<hr>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2015/10/05/als_report/" data-id="cigx3wnsl0000ikgtv3ujftsq" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/MachineLearning/">MachineLearning</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-sqoop_orc" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2015/09/11/sqoop_orc/" class="article-date">
  <time datetime="2015-09-11T07:00:11.000Z" itemprop="datePublished">2015-09-11</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2015/09/11/sqoop_orc/">使用sqoop生成ORC格式的文件，并给Hive加载</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="背景">背景</h1><p>在当前版本: sqoop-1.4.5(CDH540)中的sqoop并不支持导出格式为orcfile的格式。因此，如果我们想将数据从DB=&gt;HDFS，并生成ORC格式，只有两个办法。</p>
<p>-&gt; a) 使用HCatalog服务，目前没有测试过，看文档是可以生成ORC的格式。<br>-&gt; b) 原始办法，通过Hive的insert命令将一张原本不是ORC格式的数据，导入到另一个ORC格式中，这样就有了ORC格式的文件。 但该方法会造成时间、资源消耗翻倍。</p>
<h1 id="DIY">DIY</h1><p>既然如此，自己动手，丰衣足食。介于近年来对于sqoop的使用经验，很方便的定位到了几个地方，并加入了一些类和方法，最终达到了目的。当然，因为我们的需求目前仅仅是从DB = &gt; HDFS，即只是增加了import方法中的参数，并没有在export =&gt; DB中增加。</p>
<p>具体代码可以参考我已经提供的版本 <a href="https://github.com/3h-william/sqoop-orc-import" target="_blank" rel="external">link</a><br>使用ant编译后，替换原来的sqoop jar即可。</p>
<h1 id="关键点">关键点</h1><p>-&gt; 1) 使用hive的提供的TypeInfoUtils 构建ORC file的Schema.并传递到每一个Map中<br>-&gt; 2) 使用 org.apache.hadoop.hive.ql.io.orc.OrcNewOutputFormat 作为输出格式</p>
<hr>
<p>作者：3h-william<br>联系: <a href="https://github.com/3h-william" target="_blank" rel="external">https://github.com/3h-william</a><br>出处：<a href="http://3h-william.github.io" target="_blank" rel="external">http://3h-william.github.io</a>  </p>
<hr>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2015/09/11/sqoop_orc/" data-id="cigx3wnt80006ikgtbmxaqcdy" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/hive/">hive</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/sqoop/">sqoop</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-hbase_balance_problem" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2015/09/08/hbase_balance_problem/" class="article-date">
  <time datetime="2015-09-08T06:08:11.000Z" itemprop="datePublished">2015-09-08</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2015/09/08/hbase_balance_problem/">HBase Coprocessor问题引起异常Balance</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="问题背景">问题背景</h1><p>最近发现HBase集群的文件compaction次数明显增加。经常到时M/R job扫描snapshot文件的时候发生文件找不到，为此trace了一下日志，大致发现了问题。<br><img src="/img/hbase_balance_problems/balance_metrics.png" alt="balance统计"></p>
<h1 id="问题大致流程Trace">问题大致流程Trace</h1><ul>
<li>1)  全量索引M/R  job 失败   </li>
<li>2)  HFile文件不存在  </li>
<li>3)  HFile发生了compaction，文件被代替  </li>
<li>4)  HFile文件数量达到默认数量(3个) 触发Minor compaction  </li>
<li>5)  Memstore被flush，生成了HFile  </li>
<li>6)  HRegion 被close之前会触发flush (包括snapshot，倒到Memory的阀值也会触发)，但close是主要原因。  </li>
<li>7)  Cluster的Balance操作需要 Close &amp; Move HRegion  </li>
<li><p>8)  StochasticLoadBalancer HBase的Balance策略机制类，触发条件为:  </p>
<ul>
<li>a)  达到Balance触发条件 Max Region &gt; AVG(Region数量) +1  OR  Min Region &lt;AVG(Region数量) -1  </li>
<li>b)  调整后的Cost Function值比原来的小（策略更为优秀）<br>附:  CF的值:<br>new ReadRequestCostFunction(conf),<br>new WriteRequestCostFunction(conf),<br>new MemstoreSizeCostFunction(conf),<br>new StoreFileCostFunction(conf)</li>
</ul>
</li>
<li><p>9)  新增的Region数量并不多。不会经常触发balance</p>
</li>
<li>10)  Balance的大部分Region都是旧的Region，并且 无法调整到最优状态，导致不断调整，死循环。如下某一个时刻的Region分布:<br><img src="/img/hbase_balance_problems/regions_info.png" alt="region分布"></li>
</ul>
<p>只要Region数小于73或者大于76，就会满足条件a。<br>并且，如果机会调整之后的cost比较小，就会满足b。</p>
<ul>
<li>11)  调整不到平衡点，可能是因为某些调整失败。</li>
<li>12)  没有加载Phoenix coprocessor的机器如果balance 一些有该coprocessor的Region就会失败。</li>
<li>13)  大规模调整从9/3 18:00开始第一个balance周期。 </li>
<li>14)  9/3 17:57 分执行了truncate的操作。 </li>
</ul>
<h1 id="Balance_Trace">Balance Trace</h1><p>因为日志涉及公司信息，不再给出，大体步骤如下:<br>需要调整的Region号:  XXXXX</p>
<ul>
<li>Step 1: 将该Region从server12迁移到server24 (这台机器已dead)</li>
<li>Step 2: 因为迁移server24失败，所以选择另一台机器进行迁移，选择了server34</li>
<li>Step 3:  server34加载该Region失败，因为coprocessor的原因。</li>
<li>Step 4 : 选择server32迁移，也失败了，最终选择了server19成功</li>
</ul>
<p>Balance的总结:    </p>
<ul>
<li>a) 我们可以看到，Balance的策略是失败的，因为最初是想将这个Region迁移到server34，而最终却是迁移到了server19。  </li>
<li>b) 这样的balance，不仅仅是失败，而且会对server19增加了负担。</li>
<li>c) 从HBase代码我没有看到对这种情况的处理方式。但理论上一次失败是没有关系的，只要第二次Balance成功即可。</li>
<li>d) 但是，某些机器注定无法收容某些Region，所以，注定永远无法调整到一个平衡的balance掉。</li>
</ul>
<p>遗留问题:<br>从日志中有一个问题: server24这台机器在9/3 15:29已经dead，为何balance策略仍然会选择这台机器。是不是又bug…</p>
<hr>
<p>作者：3h-william<br>联系: <a href="https://github.com/3h-william" target="_blank" rel="external">https://github.com/3h-william</a><br>出处：<a href="http://3h-william.github.io" target="_blank" rel="external">http://3h-william.github.io</a>  </p>
<hr>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2015/09/08/hbase_balance_problem/" data-id="cigx3wnth000hikgtnpj5ckmq" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/hbase/">hbase</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-hbase-bulkload" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2015/08/27/hbase-bulkload/" class="article-date">
  <time datetime="2015-08-27T06:08:11.000Z" itemprop="datePublished">2015-08-27</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2015/08/27/hbase-bulkload/">hbase bulkload 相关整理</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>由于近期Team内有同学用到bulkload,整理了一下过往我用到的bulkload的相关使用经验<br>基于<strong>hbase 0.90</strong>和<strong>0.94</strong>版本做一些简单的分析，可供参考。  </p>
<h2 id="1-_概述"><strong>1. 概述</strong></h2><p><a href="http://hbase.apache.org/book.html#arch.bulk.load" target="_blank" rel="external">官方说明</a><br>[0.94官方说明] (<a href="http://hbase.apache.org/0.94/book/arch.bulk.load.html" target="_blank" rel="external">http://hbase.apache.org/0.94/book/arch.bulk.load.html</a>)  </p>
<p>简单的说，bluk load 是能够批量加载hfile =&gt; hbase的技术。<br>服务端，RegionServer处理客户端的bulkload请求。<br>客户端，向RegionServer发起blukload请求。  </p>
<h3 id="1-1-_案例"><strong>1.1. 案例</strong></h3><p>在2013年的maillog项目中，我们使用了bulk load技术。批量加载了从M/R生成的HFile数据。<br>因为是预先生成的HFile和处理过的，所以是一个HFile对应1个Region</p>
<h4 id="加载数据量:"><strong>加载数据量:</strong></h4><p>HFile(Region) 总数: <strong>100个</strong><br>占用磁盘大小 : <strong>450G</strong> (snappy压缩后<strong>100G</strong>)<br>消耗时间 : 在数据已经存在与HDFS的情况下，每一个HFile加载时间 <strong>小于 1秒</strong>    </p>
<h3 id="1-2_特性"><strong>1.2 特性</strong></h3><p><strong>a)</strong> bulkload 在加载的同时，并不会影响正常读写。(如果写文件造成分区变化，对bulkload有影响)<br><strong>b)</strong> bulkload 的加载是秒级的，在同一个HDFS cluster中，会将数据move。<br><strong>c)</strong> bulkload 也支持从不同的hdfs cluster copy数据(不同版本没有测试过)，同一个版本可行，它会把原来的move操作替换为copy，效率会降低。<br>(<strong>注意</strong>: 如果你的数据没有move而是copy，请check一下hdfs的URI写的是否正确一致)</p>
<h3 id="1-3-_空表和非空表的策略"><strong>1.3. 空表和非空表的策略</strong></h3><h4 id="场景定义">场景定义</h4><p>bulkload 适用于初始化整个HBase表(空表)， 也适用于对于增量数据的导入（非空表）  </p>
<h4 id="策略">策略</h4><p>显然，对于这两种场景的操作手法和策略是不同的。<br>对于任何的bulkload操作策略来说，评判目标应该是一致的: <strong>效率和影响</strong><br><strong>效率</strong> 指的是如何在短时间内将数据load。<br><strong>影响</strong> 指的是如何避免在做bulkload的时候，减少对集群的影响，避免hbase触发split/merge等heavy的操作。    </p>
<h4 id="操作手法一般包括：">操作手法一般包括：</h4><p><strong>a) HFile的预处理</strong>  通过M/R Job或者其他手段预先处理HFile，可以定义HFile大小，splitkey规则，可以在非空表的情况下，极大避免对集群的影响。<br><strong>b) 预分区</strong>  通常来说，对于空表的bulkload应该是不会触发集群的split、merge等操作的。通过预分区可以有效的规避这一点。和Hbase 的split size结合使用。 </p>
<h2 id="2-_Bulkload客户端流程"><strong>2. Bulkload客户端流程</strong></h2><p>主要围绕<strong>LoadIncrementalHFiles</strong> 类来说明基本的流程。<br>可参考我上传的代码，对比以下链接的代码<br>[0.90版本官方提供的LoadIncrementalHFiles类] (<a href="http://trgit2/wz68/importdb_blukload/blob/master/src/main/java/com/newegg/email/importdb/bulkload/tool/LoadIncrementalHFiles.java" target="_blank" rel="external">http://trgit2/wz68/importdb_blukload/blob/master/src/main/java/com/newegg/email/importdb/bulkload/tool/LoadIncrementalHFiles.java</a>)<br>[基于0.90版本修改后支持客户端并发的LoadIncrementalHFilesConcurrency类]  (<a href="http://trgit2/wz68/importdb_blukload/blob/master/src/main/java/com/newegg/email/importdb/bulkload/tool/LoadIncrementalHFilesConcurrency.java" target="_blank" rel="external">http://trgit2/wz68/importdb_blukload/blob/master/src/main/java/com/newegg/email/importdb/bulkload/tool/LoadIncrementalHFilesConcurrency.java</a>)<br>总的来说，这部分代码比较简单，客户端代码很容易读懂。</p>
<h3 id="Step_1-_main_entry">Step 1.  main entry</h3><p>  LoadIncrementalHFiles的entry方法，会接收两个参数，一个是表的名字，另一个是<strong>hfile文件或文件夹路径</strong>(此处可传递文件夹)<br>  这里的LoadIncrementalHFiles是implements于org.apache.hadoop.util.Tool，但是因为这里只是借助于tool这个工具，实质上并没有启动任何M/R job，因此我们看到的只是一个local的app启动。</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">   <span class="comment">// entry 方法</span></span><br><span class="line">   <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> throws Exception </span>&#123;</span><br><span class="line">       <span class="keyword">int</span> ret = ToolRunner.run(<span class="keyword">new</span> LoadIncrementalHFiles(HBaseConfiguration.create()), args);</span><br><span class="line">    System.<span class="built_in">exit</span>(ret);</span><br><span class="line">   &#125;</span><br><span class="line"></span><br><span class="line">   <span class="comment">// tool的默认加载方法run</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">run</span><span class="params">(String[] args)</span> throws Exception </span>&#123;</span><br><span class="line">	<span class="keyword">if</span> (args.length != <span class="number">2</span>) &#123;</span><br><span class="line">		usage();</span><br><span class="line">		<span class="keyword">return</span> -<span class="number">1</span>;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	String dirPath = args[<span class="number">0</span>];      <span class="comment">// ---&gt; 路径(文件、文件夹)</span></span><br><span class="line">	String tableName = args[<span class="number">1</span>];    <span class="comment">// ---&gt; table name</span></span><br><span class="line"></span><br><span class="line">	boolean tableExists = <span class="keyword">this</span>.doesTableExist(tableName);</span><br><span class="line">	<span class="keyword">if</span> (!tableExists)</span><br><span class="line">		<span class="keyword">this</span>.createTable(tableName, dirPath);</span><br><span class="line"></span><br><span class="line">	Path hfofDir = <span class="keyword">new</span> Path(dirPath);</span><br><span class="line">	HTable table = <span class="keyword">new</span> HTable(conf,tableName);</span><br><span class="line"></span><br><span class="line">	doBulkLoad(hfofDir, table);   <span class="comment">// ---&gt; 调用bulkload方法</span></span><br><span class="line">	<span class="keyword">return</span> <span class="number">0</span>;                     <span class="comment">// ---&gt;  Run方法中并没有调用M/R job client，因此只是一个local app，不能算是M/R的job</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="Step_2_-_串行化和并行化_doBulkLoad">Step 2 . 串行化和并行化 doBulkLoad</h3><p>串行化和并行化目前已经在高版本应该解决了。  </p>
<h4 id="a)_0-90版本的串行化:_以下这段是官方提供的代码，我们可以看到tryLoad是循环的。所以无法并发去做。"><strong>a)</strong>  0.90版本的串行化: 以下这段是官方提供的代码，我们可以看到tryLoad是循环的。所以无法并发去做。</h4><figure class="highlight openscad"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">public void doBulkLoad<span class="params">(Path hfofDir, HTable table)</span> throws TableNotFoundException, IOException &#123;</span><br><span class="line">	HConnection conn = table.getConnection<span class="params">()</span>;</span><br><span class="line"></span><br><span class="line">	<span class="keyword">if</span> <span class="params">(!conn.isTableAvailable<span class="params">(table.getTableName<span class="params">()</span>)</span>)</span> &#123;</span><br><span class="line">		throw new TableNotFoundException<span class="params">(<span class="string">"Table "</span> + Bytes.toStringBinary<span class="params">(table.getTableName<span class="params">()</span>)</span> + <span class="string">"is not currently available."</span>)</span>;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	Deque&lt;LoadQueueItem&gt; queue = null;</span><br><span class="line">	try &#123;</span><br><span class="line">		queue = discoverLoadQueue<span class="params">(hfofDir)</span>;</span><br><span class="line">		while <span class="params">(!queue.isEmpty<span class="params">()</span>)</span> &#123;                                <span class="comment">// ----&gt;  串行化消费队列，提交bulkload</span></span><br><span class="line">			LoadQueueItem item = queue.remove<span class="params">()</span>;</span><br><span class="line">			tryLoad<span class="params">(item, conn, table.getTableName<span class="params">()</span>, queue)</span>;</span><br><span class="line">		&#125;</span><br><span class="line">	&#125; finally &#123;</span><br><span class="line">		<span class="keyword">if</span> <span class="params">(queue != null &amp;&amp; !queue.isEmpty<span class="params">()</span>)</span> &#123;</span><br><span class="line">			StringBuilder err = new StringBuilder<span class="params">()</span>;</span><br><span class="line">			err.append<span class="params">(<span class="string">"-------------------------------------------------\n"</span>)</span>;</span><br><span class="line">			err.append<span class="params">(<span class="string">"Bulk load aborted with some files not yet loaded:\n"</span>)</span>;</span><br><span class="line">			err.append<span class="params">(<span class="string">"-------------------------------------------------\n"</span>)</span>;</span><br><span class="line">			<span class="keyword">for</span> <span class="params">(LoadQueueItem q : queue)</span> &#123;</span><br><span class="line">				err.append<span class="params">(<span class="string">"  "</span>)</span>.append<span class="params">(q.hfilePath)</span>.append<span class="params">('\n')</span>;</span><br><span class="line">			&#125;</span><br><span class="line">			LOG.error<span class="params">(err)</span>;</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="b)_基于0-90版本修改的并行化方案，通过构建线程池来完成。"><strong>b)</strong>  基于0.90版本修改的并行化方案，通过构建线程池来完成。</h4><figure class="highlight openscad"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">public boolean doBulkLoad<span class="params">(Path hfofDir, HTable table)</span> throws TableNotFoundException, IOException &#123;</span><br><span class="line">	boolean isSuccess=<span class="literal">false</span>;</span><br><span class="line">	HConnection conn = table.getConnection<span class="params">()</span>;</span><br><span class="line">	</span><br><span class="line">	<span class="keyword">if</span> <span class="params">(!conn.isTableAvailable<span class="params">(table.getTableName<span class="params">()</span>)</span>)</span> &#123;</span><br><span class="line">		throw new TableNotFoundException<span class="params">(<span class="string">"Table "</span> + Bytes.toStringBinary<span class="params">(table.getTableName<span class="params">()</span>)</span> + <span class="string">"is not currently available."</span>)</span>;</span><br><span class="line">	&#125;</span><br><span class="line">	List&lt;Deque&lt;LoadQueueItem&gt;&gt; queueList = null;</span><br><span class="line">	<span class="comment">// Deque&lt;LoadQueueItem&gt; queue = null;</span></span><br><span class="line">	queueList = discoverLoadQueue<span class="params">(hfofDir)</span>;</span><br><span class="line">	int threadNums=conf.getInt<span class="params">(CONCURRENCY_NUMS, <span class="number">1</span>)</span>;</span><br><span class="line">	LOG.info<span class="params">(<span class="string">"*********    thread nums = "</span>+threadNums)</span>;</span><br><span class="line">	ExecutorService executorService = Executors.newFixedThreadPool<span class="params">(threadNums)</span>;                <span class="comment">//  -----&gt; 并行化解决方案</span></span><br><span class="line">	List&lt;Future&lt;Boolean&gt;&gt; resultList = new ArrayList&lt;Future&lt;Boolean&gt;&gt;<span class="params">()</span>;</span><br><span class="line">	try&#123;</span><br><span class="line">		<span class="keyword">for</span> <span class="params">(Deque&lt;LoadQueueItem&gt; queue : queueList)</span> &#123;</span><br><span class="line">			Future&lt;Boolean&gt; future =executorService.submit<span class="params">(new LoadThread<span class="params">(queue, conn, table.getTableName<span class="params">()</span>)</span>)</span>;</span><br><span class="line">			resultList.add<span class="params">(future)</span>;</span><br><span class="line">		&#125;</span><br><span class="line">		<span class="keyword">for</span> <span class="params">(Future&lt;Boolean&gt; result : resultList)</span> &#123;</span><br><span class="line">			<span class="keyword">if</span> <span class="params">(!result.get<span class="params">(<span class="number">100</span>, TimeUnit.MINUTES)</span>)</span> &#123;</span><br><span class="line">				LOG.error<span class="params">(<span class="string">"validatorThread return false"</span>)</span>;</span><br><span class="line">				return <span class="literal">false</span>;</span><br><span class="line">			&#125;</span><br><span class="line">		&#125;</span><br><span class="line">		isSuccess = <span class="literal">true</span>;</span><br><span class="line">	&#125;catch<span class="params">(Exception e)</span>&#123;</span><br><span class="line">		LOG.error<span class="params">(e)</span>;</span><br><span class="line">	&#125;finally&#123;</span><br><span class="line">		executorService.shutdown<span class="params">()</span>;</span><br><span class="line">		return isSuccess;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="c)_官方0-94的版本的代码片段，已经提供了并行化，并且大幅修改了代码，开启并行化，需要手动设置参数:_hbase-loadincremental-threads-max，默认值是机器的cpu_cores相关"><strong>c)</strong> 官方0.94的版本的代码片段，已经提供了并行化，并且大幅修改了代码，开启并行化，需要手动设置参数: <strong>hbase.loadincremental.threads.max</strong>，默认值是机器的cpu cores相关</h4><figure class="highlight openscad"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">public void doBulkLoad<span class="params">(Path hfofDir, final HTable table)</span></span><br><span class="line">   throws TableNotFoundException, IOException</span><br><span class="line"> &#123;</span><br><span class="line">   final HConnection conn = table.getConnection<span class="params">()</span>;</span><br><span class="line"></span><br><span class="line">   <span class="keyword">if</span> <span class="params">(!conn.isTableAvailable<span class="params">(table.getTableName<span class="params">()</span>)</span>)</span> &#123;</span><br><span class="line">     throw new TableNotFoundException<span class="params">(<span class="string">"Table "</span> +</span><br><span class="line">         Bytes.toStringBinary<span class="params">(table.getTableName<span class="params">()</span>)</span> +</span><br><span class="line">         <span class="string">"is not currently available."</span>)</span>;</span><br><span class="line">   &#125;</span><br><span class="line"></span><br><span class="line">   <span class="comment">// initialize thread pools                                                        ----&gt;  并行化处理加载HFile</span></span><br><span class="line">   int nrThreads = cfg.getInt<span class="params">(<span class="string">"hbase.loadincremental.threads.max"</span>,</span><br><span class="line">       Runtime.getRuntime<span class="params">()</span>.availableProcessors<span class="params">()</span>)</span>;</span><br><span class="line">   ThreadFactoryBuilder builder = new ThreadFactoryBuilder<span class="params">()</span>;</span><br><span class="line">   builder.setNameFormat<span class="params">(<span class="string">"LoadIncrementalHFiles-%1$d"</span>)</span>;</span><br><span class="line">   ExecutorService pool = new ThreadPoolExecutor<span class="params">(nrThreads, nrThreads,</span><br><span class="line">       <span class="number">60</span>, TimeUnit.SECONDS,</span><br><span class="line">       new LinkedBlockingQueue&lt;Runnable&gt;<span class="params">()</span>,</span><br><span class="line">       builder.build<span class="params">()</span>)</span>;</span><br><span class="line">   <span class="params">(<span class="params">(ThreadPoolExecutor)</span>pool)</span>.allowCoreThreadTimeOut<span class="params">(<span class="literal">true</span>)</span>;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">   ......</span><br></pre></td></tr></table></figure>
<h3 id="Step_3-_查看hfile边界，以决定是否需要split">Step 3. 查看hfile边界，以决定是否需要split</h3><p>这个地方就是是否会在客户端发生split的原因<br>直接参考0.94的代码<br><figure class="highlight openscad"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span><br><span class="line">   * Attempt to assign the given load queue item into its target region group.</span><br><span class="line">   * If the hfile boundary no longer fits into a region, physically splits</span><br><span class="line">   * the hfile such that the new bottom half will fit and returns the list of</span><br><span class="line">   * LQI's corresponding to the resultant hfiles.</span><br><span class="line">   *</span><br><span class="line">   * protected for testing</span><br><span class="line">   */</span></span><br><span class="line">  protected List&lt;LoadQueueItem&gt; groupOrSplit<span class="params">(Multimap&lt;ByteBuffer, LoadQueueItem&gt; regionGroups,</span><br><span class="line">      final LoadQueueItem item, final HTable table,</span><br><span class="line">      final Pair&lt;byte[][], byte[][]&gt; startEndKeys)</span></span><br><span class="line">      throws IOException &#123;</span><br><span class="line">    final Path hfilePath = item.hfilePath;</span><br><span class="line">    final FileSystem fs = hfilePath.getFileSystem<span class="params">(getConf<span class="params">()</span>)</span>;</span><br><span class="line">    HFile.Reader hfr = HFile.createReader<span class="params">(fs, hfilePath,</span><br><span class="line">        new CacheConfig<span class="params">(getConf<span class="params">()</span>)</span>)</span>;</span><br><span class="line">    final byte[] first, last;</span><br><span class="line">    try &#123;</span><br><span class="line">      hfr.loadFileInfo<span class="params">()</span>;</span><br><span class="line">      first = hfr.getFirstRowKey<span class="params">()</span>;</span><br><span class="line">      last = hfr.getLastRowKey<span class="params">()</span>;</span><br><span class="line">    &#125;  finally &#123;</span><br><span class="line">      hfr.close<span class="params">()</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    LOG.info<span class="params">(<span class="string">"Trying to load hfile="</span> + hfilePath +                       ----&gt; 从hfile中得到first 和 last的信息</span><br><span class="line">        <span class="string">" first="</span> + Bytes.toStringBinary<span class="params">(first)</span> +</span><br><span class="line">        <span class="string">" last="</span>  + Bytes.toStringBinary<span class="params">(last)</span>)</span>;</span><br><span class="line">    <span class="keyword">if</span> <span class="params">(first == null || last == null)</span> &#123;</span><br><span class="line">      assert first == null &amp;&amp; last == null;</span><br><span class="line">      <span class="comment">// TODO what if this is due to a bad HFile?</span></span><br><span class="line">      LOG.info<span class="params">(<span class="string">"hfile "</span> + hfilePath + <span class="string">" has no entries, skipping"</span>)</span>;</span><br><span class="line">      return null;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> <span class="params">(Bytes.compareTo<span class="params">(first, last)</span> &gt; <span class="number">0</span>)</span> &#123;                              </span><br><span class="line">      throw new IllegalArgumentException<span class="params">(</span><br><span class="line">      <span class="string">"Invalid range: "</span> + Bytes.toStringBinary<span class="params">(first)</span> +</span><br><span class="line">      <span class="string">" &gt; "</span> + Bytes.toStringBinary<span class="params">(last)</span>)</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    int idx = Arrays.binarySearch<span class="params">(startEndKeys.getFirst<span class="params">()</span>, first,        ---&gt;  从整个集群的statEndKeys的多列中搜索是否满足该HFile的Region</span><br><span class="line">        Bytes.BYTES_COMPARATOR)</span>;</span><br><span class="line">    <span class="keyword">if</span> <span class="params">(idx &lt; <span class="number">0</span>)</span> &#123;</span><br><span class="line">      <span class="comment">// not on boundary, returns -(insertion index).  Calculate region it</span></span><br><span class="line">      <span class="comment">// would be in.</span></span><br><span class="line">      idx = -<span class="params">(idx + <span class="number">1</span>)</span> - <span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    final int indexForCallable = idx;</span><br><span class="line">    boolean lastKeyInRange =</span><br><span class="line">      Bytes.compareTo<span class="params">(last, startEndKeys.getSecond<span class="params">()</span>[idx])</span> &lt; <span class="number">0</span> ||</span><br><span class="line">      Bytes.equals<span class="params">(startEndKeys.getSecond<span class="params">()</span>[idx], HConstants.EMPTY_BYTE_ARRAY)</span>;</span><br><span class="line">    <span class="keyword">if</span> <span class="params">(!lastKeyInRange)</span> &#123;</span><br><span class="line">      List&lt;LoadQueueItem&gt; lqis = splitStoreFile<span class="params">(item, table,             ---&gt;  如果不满足，就会触发客户端split，调用splitStoreFile方法这里就时效率慢的原因！！！！</span><br><span class="line">          startEndKeys.getFirst<span class="params">()</span>[indexForCallable],</span><br><span class="line">          startEndKeys.getSecond<span class="params">()</span>[indexForCallable])</span>;</span><br><span class="line">      return lqis;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// group regions.</span></span><br><span class="line">    regionGroups.put<span class="params">(ByteBuffer.wrap<span class="params">(startEndKeys.getFirst<span class="params">()</span>[idx])</span>, item)</span>;         ---&gt; 如果HFile满足区间，或者是已经将不满足区间的HFile切分满足，那么就加到RegionGroup中，等待加载.</span><br><span class="line">    return null;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure></p>
<h3 id="Step_4-_客户端split的两个方法">Step 4. 客户端split的两个方法</h3><figure class="highlight processing"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 决定split的范围，数量，分区</span></span><br><span class="line">  <span class="keyword">protected</span> List&lt;LoadQueueItem&gt; splitStoreFile(<span class="keyword">final</span> LoadQueueItem item,</span><br><span class="line">      <span class="keyword">final</span> HTable table, <span class="built_in">byte</span>[] startKey,</span><br><span class="line">      <span class="built_in">byte</span>[] splitKey) <span class="keyword">throws</span> IOException &#123;</span><br><span class="line">    <span class="keyword">final</span> Path hfilePath = item.hfilePath;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// We use a '_' prefix which is ignored when walking directory trees</span></span><br><span class="line">    <span class="comment">// above.</span></span><br><span class="line">    <span class="keyword">final</span> Path tmpDir = <span class="keyword">new</span> Path(item.hfilePath.getParent(), <span class="string">"_tmp"</span>);</span><br><span class="line"></span><br><span class="line">    LOG.info(<span class="string">"HFile at "</span> + hfilePath + <span class="string">" no longer fits inside a single "</span> +</span><br><span class="line">    <span class="string">"region. Splitting..."</span>);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">String</span> uniqueName = getUniqueName(table.getTableName());</span><br><span class="line">    HColumnDescriptor familyDesc = table.getTableDescriptor().getFamily(item.family);</span><br><span class="line">    Path botOut = <span class="keyword">new</span> Path(tmpDir, uniqueName + <span class="string">".bottom"</span>);</span><br><span class="line">    Path topOut = <span class="keyword">new</span> Path(tmpDir, uniqueName + <span class="string">".top"</span>);</span><br><span class="line">    splitStoreFile(getConf(), hfilePath, familyDesc, splitKey,</span><br><span class="line">        botOut, topOut);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Add these back at the *front* of the queue, so there's a lower</span></span><br><span class="line">    <span class="comment">// chance that the region will just split again before we get there.</span></span><br><span class="line">    List&lt;LoadQueueItem&gt; lqis = <span class="keyword">new</span> ArrayList&lt;LoadQueueItem&gt;(<span class="number">2</span>);</span><br><span class="line">    lqis.<span class="built_in">add</span>(<span class="keyword">new</span> LoadQueueItem(item.family, botOut));</span><br><span class="line">    lqis.<span class="built_in">add</span>(<span class="keyword">new</span> LoadQueueItem(item.family, topOut));</span><br><span class="line"></span><br><span class="line">    LOG.info(<span class="string">"Successfully split into new HFiles "</span> + botOut + <span class="string">" and "</span> + topOut);      <span class="comment">// 如果split成功，我们会看到这些日志...</span></span><br><span class="line">    <span class="keyword">return</span> lqis;</span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line">  </span><br><span class="line">  <span class="comment">// 真正调用执行split,copy....</span></span><br><span class="line">  </span><br><span class="line">    <span class="comment">/**</span><br><span class="line">   * Split a storefile into a top and bottom half, maintaining</span><br><span class="line">   * the metadata, recreating bloom filters, etc.</span><br><span class="line">   */</span></span><br><span class="line">  <span class="keyword">static</span> <span class="keyword">void</span> splitStoreFile(</span><br><span class="line">      Configuration conf, Path inFile,</span><br><span class="line">      HColumnDescriptor familyDesc, <span class="built_in">byte</span>[] splitKey,</span><br><span class="line">      Path bottomOut, Path topOut) <span class="keyword">throws</span> IOException</span><br><span class="line">  &#123;</span><br><span class="line">    <span class="comment">// Open reader with no block cache, and not in-memory</span></span><br><span class="line">    Reference topReference = <span class="keyword">new</span> Reference(splitKey, Range.top);</span><br><span class="line">    Reference bottomReference = <span class="keyword">new</span> Reference(splitKey, Range.bottom);</span><br><span class="line"></span><br><span class="line">    copyHFileHalf(conf, inFile, topOut, topReference, familyDesc);                    -----&gt; 客户端<span class="built_in">split</span>的真正<span class="built_in">copy</span>的方法</span><br><span class="line">    copyHFileHalf(conf, inFile, bottomOut, bottomReference, familyDesc); </span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
<h2 id="3-_Q&amp;A"><strong>3. Q&amp;A</strong></h2><h3 id="1)_Q:_目前的工具是否是并行化加载?"><strong>1)  Q:  目前的工具是否是并行化加载?</strong></h3><pre><code>通过上面的分析，我们可以看到，从0.94开始就已经是有并行化处理了，因此我们不需要再像我之前的那种方式去做了。只要定义好自己的线程池大小就ok.   
但我们也需要注意的是，由于<span class="keyword">*</span><span class="keyword">*</span>LoadIncrementalHFiles<span class="keyword">*</span><span class="keyword">*</span> 工具并不是M/R job，所以不要指望能直接通过M/R来提高效率。  
</code></pre><h3 id="2)_Q:_如果避免split?"><strong>2)  Q:  如果避免split?</strong></h3><pre><code><span class="built_in">split</span>有两个定义。一个是客户端的<span class="built_in">split</span>，另一个是Region Server的<span class="built_in">split</span>。  
在空表情况下，通过预分区的方式，只要<span class="variable">key</span>分的准确，就不会触发客户端的<span class="built_in">split</span>。结合hbase 原本的 <span class="built_in">split</span> <span class="built_in">size</span> 参数，使HFile对应的HRegion大小合适，不会造成RSr的<span class="built_in">split</span>.
</code></pre><h3 id="3)_Q:_对空表的bulkload的过程中，可否正常读写?"><strong>3)  Q:  对空表的bulkload的过程中，可否正常读写?</strong></h3><pre><code>读肯定是没有问题的，写的话，如果数据量不大，不触发Region的<span class="built_in">split</span>，则也没有关系。  
</code></pre><h3 id="4)_Q:_为什么bulkload的过程中，在数据已经准备的情况下，效率那么快，秒级别加载，并且不会disable_Region?"><strong>4)  Q:  为什么bulkload的过程中，在数据已经准备的情况下，效率那么快，秒级别加载，并且不会disable Region?</strong></h3><pre><code>前面只分析了Bulkload的客户端代码，如果去翻阅一下服务端代码我们可以知道。对于<span class="keyword">RS</span>来说，bulkload就两件事情，第一件是mv文件，第二件是在<span class="keyword">RS</span>的memory里加上新HFile的meta信息。  
</code></pre><h3 id="5)_Q:_对于非空表的增量数据bulkload，基本策略有哪些?"><strong>5)  Q:  对于非空表的增量数据bulkload，基本策略有哪些?</strong></h3><pre><code>参考步骤如下:  
a)  对新增HFiles大小做预估计，如果当前<span class="keyword">cluster</span>可以容纳，则不需要预分区，否则，可以先预分区。  
b)  通过<span class="keyword">M</span>/R对HFile做符合当前集群分区情况进行切分。  
c)  加载。  
</code></pre><p>对此，官方设计中有一段说明:<br>If the region boundaries have changed during the course of bulk load preparation, or between the preparation and completion steps, the completebulkloads utility will automatically split the data files into pieces corresponding to the new boundaries. This process is not optimally efficient, so users should take care to minimize the delay between preparing a bulk load and importing it into the cluster, especially if other clients are simultaneously loading data through other means.</p>
<p>大致意思是，即使客户端工具能够完成split的工作，但并不是最有效的方式，用户应该注意边界问题。</p>
<hr>
<p>作者：3h-william<br>联系: <a href="https://github.com/3h-william" target="_blank" rel="external">https://github.com/3h-william</a><br>出处：<a href="http://3h-william.github.io" target="_blank" rel="external">http://3h-william.github.io</a>  </p>
<hr>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2015/08/27/hbase-bulkload/" data-id="cigx3wntn000jikgt3epy5ha1" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/hbase/">hbase</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-elasticsearch_dc" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2015/08/10/elasticsearch_dc/" class="article-date">
  <time datetime="2015-08-10T01:50:22.000Z" itemprop="datePublished">2015-08-10</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2015/08/10/elasticsearch_dc/">为ElasticSearch 添加支持跨DC的特性</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="背景">背景</h1><p>无意间发现，去年年初提到社区的改动，在去年8月得到回复，虽然没有被采纳…<br><a href="https://github.com/elastic/elasticsearch/pull/4651#issuecomment-51578376" target="_blank" rel="external">https://github.com/elastic/elasticsearch/pull/4651#issuecomment-51578376</a><br>对方提出两点:</p>
<ol>
<li>不赞成ElasticSearch跨越数据中心构建。   </li>
<li>认为同步和延迟会造成比较大的问题。    </li>
</ol>
<h3 id="不过，目前为止，我认为这个特性有它的价值，所以在此分享一些当初的设计思想和改造方法。同时，透露一下，这个改造在公司内部也正常稳定运行了近一年多"><strong>不过，目前为止，我认为这个特性有它的价值，所以在此分享一些当初的设计思想和改造方法。同时，透露一下，这个改造在公司内部也正常稳定运行了近一年多</strong></h3><h1 id="改进点">改进点</h1><p>Elastic Search的节点设计思想是，每一组索引分片，由一个primary做write操作，而前天的replica做读操作。<br>那么，如果对于一个跨数据中心的elastic Search设计，将会因为以下的场景，带来一些问题:</p>
<ol>
<li>如果所有的写操作是由一个数据中心完成（事实上，大多数情况都是如此），由一个主的数据中心处理数据，然后分发到其他的数据中心(跨location)</li>
<li>而ES的Primary的规则，并不支持将Primary都分配在同属于一个location的Node，而是会根据其自身的集中策略来分配</li>
<li>根据以上两点，最终的结果是: 如果由两个数据中心，Primary可能分配到任何的数据中心，造成write操作会分散在不同的location</li>
</ol>
<h3 id="而我们要做的是，提供一种策略，使得primary分配到同一个location下的index分片"><strong>而我们要做的是，提供一种策略，使得primary分配到同一个location下的index分片</strong></h3><h1 id="改进效果">改进效果</h1><p><img src="/img/elasticsearch_dc/1.png" alt="分布"></p>
<h3 id="这种策略可以支持任何的情况:">这种策略可以支持任何的情况:</h3><ol>
<li>primary所在的location全部down掉，只剩下replica的location</li>
<li>任意顺序启动Node.</li>
<li>任意一种HA的可能情况。  </li>
</ol>
<h3 id="代码部分，可参考我已经提交的版本">代码部分，可参考我已经提交的版本</h3><p><a href="https://github.com/3h-william/elasticsearch-dc-version0.0.1" target="_blank" rel="external">https://github.com/3h-william/elasticsearch-dc-version0.0.1</a></p>
<hr>
<p>作者：3h-william<br>联系: <a href="https://github.com/3h-william" target="_blank" rel="external">https://github.com/3h-william</a><br>出处：<a href="http://3h-william.github.io" target="_blank" rel="external">http://3h-william.github.io</a>  </p>
<hr>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2015/08/10/elasticsearch_dc/" data-id="cigx3wntq000likgtfg3hlq9z" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/elasticsearch/">elasticsearch</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/search/">search</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-hbase_tsdb_issue" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2015/06/07/hbase_tsdb_issue/" class="article-date">
  <time datetime="2015-06-07T01:08:11.000Z" itemprop="datePublished">2015-06-07</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2015/06/07/hbase_tsdb_issue/">Open Tsdb 引起 HBase的严重问题</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="背景">背景</h1><p>前一段时间，内部用到TSDB，发现经常引起HBase服务挂掉。最终，我这里经过一些排查，找到了问题。<br>并且，我们已经向社区提出了这个问题。在这里，以此记录解决过程。  </p>
<h1 id="一-_根本原因:">一. 根本原因:</h1><p>因为TSDB的合并导致单条记录的column非常大。一旦大于MAX_CHUNK_SIZE_KEY = “hfile.index.block.max.size”(默认128K);  就由一定几率使得Memstore  Flush无限进行。<br>并且本地已经模拟出该场景。我插入了一条Column非常大的记录(150K) ，然后调用flush， RS进入死循环，HDFS文件不断增大.</p>
<p><img src="/img/tsdb_hbase_issue/1.jpg" alt=""></p>
<h1 id="二-_大致分析流程如下">二. 大致分析流程如下</h1><p>1.今天再次走代码流程和日志记录，纠正了昨天判断是HBase Compaction触发导致的问题。(出问题的那个点，并没有进行HBase层面的Compact)<br>2.进一步确认，是因为HBase的Memstore Flush成HFile导致。 （Hfile也是存在于Region/.tmp中,在一个时间点,HBase都做了一次flush<br>3.随后定位到，问题出在了Flush Memstore的时候，需要生产HFile V2 下的BlockData Index, 并导致写索引的死循环。  </p>
<h2 id="接着，让我们来看看，代码究竟做了些什么事情:">接着，让我们来看看，代码究竟做了些什么事情:</h2><p>a)选取第一条KV，作为RootChunk，建立索引<br><img src="/img/tsdb_hbase_issue/2.jpg" alt=""><br>b)写完Level Two的索引数据，但是还是要将Parent的索引写到Level One的索引中<br><img src="/img/tsdb_hbase_issue/3.jpg" alt=""><br>c)进入a图中的死循环。 反复调用写Root KV的索引，但都因为length太大，再次写子索引，周而复始，至死不休。</p>
<h1 id="三-_社区反映">三. 社区反映</h1><p><a href="https://issues.apache.org/jira/browse/HBASE-12320" target="_blank" rel="external">https://issues.apache.org/jira/browse/HBASE-12320</a><br>这是去年在JIRA 上提出的 ISSUE，但一直都没有得到解决。虽然已经有人意识到了问题。<br>事实上，不论是ColumnName的大小，或是Rowkey的大小，或是Faimily的大小，一旦超过这个阀值，就会出问题。<br>同时，只是建议，HBase的单个KV的大小，不超过128K。   </p>
<h1 id="四-_解决方案">四. 解决方案</h1><p>注意TSDB的推送的数据，不要分的太小。或者，关闭compaction(TSDB层面)  </p>
<hr>
<p>作者：3h-william<br>联系: <a href="https://github.com/3h-william" target="_blank" rel="external">https://github.com/3h-william</a><br>出处：<a href="http://3h-william.github.io" target="_blank" rel="external">http://3h-william.github.io</a>  </p>
<hr>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2015/06/07/hbase_tsdb_issue/" data-id="cigx3wntd000cikgtv20kfg1i" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/hbase/">hbase</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/tsdb/">tsdb</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-hello-world" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2015/06/01/hello-world/" class="article-date">
  <time datetime="2015-06-01T06:08:11.000Z" itemprop="datePublished">2015-06-01</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2015/06/01/hello-world/">Hello World , Hello PPG , Hello Sunshine</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>You are always my sunshine…</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2015/06/01/hello-world/" data-id="cigx3wntb000bikgtmzn4ftkm" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
  
</section>
        
          <aside id="sidebar">
  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list"><li class="tag-list-item"><a class="tag-list-link" href="/tags/MachineLearning/">MachineLearning</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/elasticsearch/">elasticsearch</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/hbase/">hbase</a><span class="tag-list-count">3</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/hive/">hive</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/search/">search</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/sqoop/">sqoop</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/tsdb/">tsdb</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/zookeeper/">zookeeper</a><span class="tag-list-count">1</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/MachineLearning/" style="font-size: 10px;">MachineLearning</a> <a href="/tags/elasticsearch/" style="font-size: 10px;">elasticsearch</a> <a href="/tags/hbase/" style="font-size: 20px;">hbase</a> <a href="/tags/hive/" style="font-size: 10px;">hive</a> <a href="/tags/search/" style="font-size: 10px;">search</a> <a href="/tags/sqoop/" style="font-size: 10px;">sqoop</a> <a href="/tags/tsdb/" style="font-size: 10px;">tsdb</a> <a href="/tags/zookeeper/" style="font-size: 10px;">zookeeper</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/11/">November 2015</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/10/">October 2015</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/09/">September 2015</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/08/">August 2015</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/06/">June 2015</a><span class="archive-list-count">2</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recents</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2015/11/10/zookeeper_stuck_dns_network/">使用Zookeeper过程中遇到客户端stuck的问题和解决建议</a>
          </li>
        
          <li>
            <a href="/2015/10/05/als_report/">推荐算法与协同过滤</a>
          </li>
        
          <li>
            <a href="/2015/09/11/sqoop_orc/">使用sqoop生成ORC格式的文件，并给Hive加载</a>
          </li>
        
          <li>
            <a href="/2015/09/08/hbase_balance_problem/">HBase Coprocessor问题引起异常Balance</a>
          </li>
        
          <li>
            <a href="/2015/08/27/hbase-bulkload/">hbase bulkload 相关整理</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2015 3h_william<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css" type="text/css">
  <script src="/fancybox/jquery.fancybox.pack.js" type="text/javascript"></script>


<script src="/js/script.js" type="text/javascript"></script>

  </div>
</body>
</html>